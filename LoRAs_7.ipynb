{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in lora1_tensors:\n",
      "lora_te_text_model_encoder_layers_0_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_0_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_0_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_0_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_0_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_0_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_0_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_0_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_0_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_0_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_0_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_0_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_0_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_0_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_0_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_0_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_0_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_0_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_10_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_10_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_10_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_10_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_10_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_10_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_10_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_10_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_10_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_10_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_10_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_10_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_10_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_10_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_10_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_10_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_10_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_10_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_11_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_11_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_11_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_11_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_11_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_11_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_11_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_11_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_11_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_11_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_11_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_11_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_11_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_11_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_11_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_11_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_11_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_11_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_1_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_1_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_1_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_1_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_1_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_1_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_1_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_1_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_1_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_1_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_1_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_1_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_1_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_1_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_1_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_1_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_1_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_1_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_2_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_2_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_2_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_2_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_2_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_2_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_2_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_2_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_2_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_2_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_2_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_2_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_2_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_2_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_2_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_2_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_2_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_2_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_3_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_3_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_3_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_3_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_3_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_3_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_3_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_3_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_3_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_3_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_3_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_3_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_3_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_3_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_3_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_3_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_3_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_3_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_4_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_4_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_4_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_4_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_4_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_4_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_4_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_4_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_4_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_4_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_4_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_4_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_4_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_4_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_4_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_4_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_4_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_4_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_5_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_5_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_5_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_5_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_5_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_5_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_5_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_5_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_5_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_5_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_5_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_5_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_5_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_5_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_5_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_5_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_5_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_5_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_6_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_6_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_6_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_6_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_6_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_6_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_6_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_6_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_6_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_6_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_6_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_6_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_6_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_6_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_6_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_6_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_6_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_6_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_7_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_7_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_7_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_7_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_7_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_7_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_7_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_7_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_7_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_7_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_7_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_7_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_7_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_7_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_7_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_7_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_7_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_7_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_8_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_8_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_8_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_8_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_8_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_8_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_8_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_8_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_8_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_8_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_8_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_8_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_8_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_8_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_8_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_8_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_8_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_8_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_9_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_9_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_9_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_9_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_9_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_9_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_9_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_9_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_9_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_9_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_9_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_9_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_9_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_9_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_9_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_9_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_9_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_9_self_attn_v_proj.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_proj_in.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_proj_in.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_proj_in.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_proj_out.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_proj_out.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_proj_out.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_proj_in.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_proj_in.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_proj_in.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_proj_out.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_proj_out.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_proj_out.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_proj_in.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_proj_in.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_proj_in.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_proj_out.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_proj_out.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_proj_out.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_proj_in.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_proj_in.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_proj_in.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_proj_out.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_proj_out.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_proj_out.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_proj_in.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_proj_in.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_proj_in.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_proj_out.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_proj_out.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_proj_out.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_proj_in.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_proj_in.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_proj_in.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_proj_out.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_proj_out.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_proj_out.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_proj_in.alpha\n",
      "lora_unet_mid_block_attentions_0_proj_in.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_proj_in.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_proj_out.alpha\n",
      "lora_unet_mid_block_attentions_0_proj_out.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_proj_out.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_proj_in.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_proj_in.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_proj_in.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_proj_out.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_proj_out.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_proj_out.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_proj_in.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_proj_in.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_proj_in.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_proj_out.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_proj_out.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_proj_out.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_proj_in.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_proj_in.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_proj_in.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_proj_out.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_proj_out.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_proj_out.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_proj_in.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_proj_in.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_proj_in.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_proj_out.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_proj_out.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_proj_out.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_proj_in.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_proj_in.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_proj_in.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_proj_out.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_proj_out.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_proj_out.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_proj_in.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_proj_in.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_proj_in.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_proj_out.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_proj_out.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_proj_out.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_proj_in.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_proj_in.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_proj_in.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_proj_out.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_proj_out.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_proj_out.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_proj_in.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_proj_in.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_proj_in.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_proj_out.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_proj_out.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_proj_out.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_proj_in.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_proj_in.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_proj_in.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_proj_out.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_proj_out.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_proj_out.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2.lora_up.weight\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from safetensors import safe_open\n",
    "from scipy.linalg import svd\n",
    "import numpy as np\n",
    "\n",
    "# Load the LoRA tensors from .safetensors files\n",
    "with safe_open(\"lora1.safetensors\", framework=\"pt\", device=\"cpu\") as f:\n",
    "    lora1_tensors = {}\n",
    "    for k in f.keys():\n",
    "        lora1_tensors[k] = f.get_tensor(k)\n",
    "\n",
    "# Print the available keys\n",
    "print(\"Keys in lora1_tensors:\")\n",
    "for key in lora1_tensors.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Keys in lora2_tensors:\n",
      "lora_te_text_model_encoder_layers_0_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_0_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_0_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_0_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_0_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_0_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_0_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_0_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_0_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_0_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_0_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_0_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_0_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_0_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_0_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_0_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_0_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_0_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_10_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_10_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_10_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_10_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_10_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_10_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_10_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_10_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_10_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_10_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_10_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_10_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_10_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_10_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_10_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_10_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_10_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_10_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_11_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_11_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_11_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_11_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_11_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_11_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_11_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_11_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_11_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_11_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_11_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_11_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_11_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_11_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_11_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_11_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_11_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_11_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_1_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_1_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_1_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_1_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_1_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_1_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_1_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_1_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_1_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_1_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_1_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_1_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_1_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_1_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_1_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_1_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_1_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_1_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_2_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_2_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_2_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_2_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_2_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_2_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_2_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_2_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_2_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_2_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_2_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_2_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_2_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_2_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_2_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_2_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_2_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_2_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_3_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_3_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_3_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_3_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_3_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_3_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_3_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_3_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_3_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_3_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_3_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_3_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_3_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_3_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_3_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_3_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_3_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_3_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_4_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_4_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_4_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_4_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_4_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_4_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_4_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_4_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_4_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_4_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_4_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_4_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_4_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_4_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_4_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_4_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_4_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_4_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_5_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_5_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_5_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_5_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_5_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_5_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_5_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_5_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_5_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_5_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_5_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_5_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_5_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_5_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_5_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_5_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_5_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_5_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_6_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_6_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_6_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_6_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_6_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_6_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_6_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_6_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_6_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_6_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_6_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_6_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_6_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_6_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_6_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_6_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_6_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_6_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_7_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_7_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_7_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_7_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_7_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_7_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_7_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_7_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_7_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_7_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_7_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_7_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_7_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_7_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_7_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_7_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_7_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_7_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_8_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_8_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_8_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_8_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_8_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_8_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_8_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_8_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_8_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_8_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_8_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_8_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_8_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_8_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_8_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_8_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_8_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_8_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_9_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_9_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_9_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_9_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_9_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_9_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_9_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_9_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_9_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_9_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_9_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_9_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_9_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_9_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_9_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_9_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_9_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_9_self_attn_v_proj.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_proj_in.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_proj_in.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_proj_in.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_proj_out.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_proj_out.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_proj_out.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_proj_in.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_proj_in.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_proj_in.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_proj_out.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_proj_out.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_proj_out.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_proj_in.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_proj_in.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_proj_in.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_proj_out.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_proj_out.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_proj_out.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_proj_in.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_proj_in.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_proj_in.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_proj_out.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_proj_out.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_proj_out.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_proj_in.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_proj_in.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_proj_in.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_proj_out.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_proj_out.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_proj_out.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_proj_in.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_proj_in.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_proj_in.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_proj_out.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_proj_out.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_proj_out.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_proj_in.alpha\n",
      "lora_unet_mid_block_attentions_0_proj_in.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_proj_in.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_proj_out.alpha\n",
      "lora_unet_mid_block_attentions_0_proj_out.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_proj_out.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_proj_in.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_proj_in.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_proj_in.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_proj_out.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_proj_out.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_proj_out.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_proj_in.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_proj_in.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_proj_in.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_proj_out.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_proj_out.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_proj_out.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_proj_in.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_proj_in.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_proj_in.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_proj_out.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_proj_out.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_proj_out.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_proj_in.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_proj_in.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_proj_in.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_proj_out.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_proj_out.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_proj_out.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_proj_in.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_proj_in.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_proj_in.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_proj_out.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_proj_out.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_proj_out.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_proj_in.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_proj_in.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_proj_in.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_proj_out.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_proj_out.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_proj_out.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_proj_in.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_proj_in.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_proj_in.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_proj_out.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_proj_out.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_proj_out.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_proj_in.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_proj_in.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_proj_in.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_proj_out.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_proj_out.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_proj_out.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_proj_in.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_proj_in.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_proj_in.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_proj_out.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_proj_out.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_proj_out.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2.lora_up.weight\n"
     ]
    }
   ],
   "source": [
    "with safe_open(\"lora2.safetensors\", framework=\"pt\", device=\"cpu\") as f:\n",
    "    lora2_tensors = {}\n",
    "    for k in f.keys():\n",
    "        lora2_tensors[k] = f.get_tensor(k)\n",
    "\n",
    "print(\"\\nKeys in lora2_tensors:\")\n",
    "for key in lora2_tensors.keys():\n",
    "    print(key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in the paper on LoRAs, the following concept is introduced: The subspace similarity measure is a way of measuring the similarity between the subspaces spanned by the top singular vectors of two low-rank adaptation matrices, $A_{r=8}$ and $A_{r=64}$, from the same pre-trained model. Here's how it's done:\n",
    "\n",
    "First, you perform a singular value decomposition (SVD) on each of these matrices to obtain their right-singular unitary matrices, denoted $U_{A_{r=8}}$ and $U_{A_{r=64}}$.\n",
    "\n",
    "The goal is then to quantify how much of the subspace spanned by the top $i$ singular vectors in $U_{A_{r=8}}$ is contained in the subspace spanned by the top $j$ singular vectors of $U_{A_{r=64}}$.\n",
    "\n",
    "This is measured using a normalized subspace similarity based on the Grassmann distance. The formula for this measure, denoted $\\phi(A_{r=8}, A_{r=64}, i, j)$, is given as follows:\n",
    "\n",
    "$$\n",
    "\\phi(A_{r=8}, A_{r=64}, i, j) = \\frac{||U_{A_{r=8}}^{(i)} {U_{A_{r=64}}^{(j)}}^T||_F^2}{\\min(i, j)}\n",
    "$$\n",
    "\n",
    "where $U_{A_{r}}^{(i)}$ represents the columns of $U_{A_{r}}$ corresponding to the top $i$ singular vectors, and $||\\cdot||_F$ denotes the Frobenius norm.\n",
    "\n",
    "The measure $\\phi(·)$ ranges from 0 to 1, where 1 represents a complete overlap of the subspaces (i.e., they are the same), and 0 represents a complete separation (i.e., they are orthogonal). This is a normalized measure because it's divided by $\\min(i, j)$, which is the maximum possible square of the Frobenius norm of the product matrix $U_{A_{r=8}}^{(i)} {U_{A_{r=64}}^{(j)}}^T$.\n",
    "\n",
    "This process is performed for all pairs $(i, j)$ where $1 \\leq i \\leq 8$ and $1 \\leq j \\leq 64$. The results give an understanding of how much the learned subspaces for different ranks overlap with each other.\n",
    "\n",
    "This can also be performed on two layers $\\Delta W_1 = B_1A_1$ and $\\Delta W_2 = B_2A_2$  in two different LoRAs. In particular, suppose we choose a layer `n` of each LoRA and run the subspace similarity measure comparison on $U_{\\Delta W_1}^{(i)} {U_{\\Delta W_2}^{(j)}}^T$. Then this will tell us how much those to LoRAs overlap with one another. \n",
    "\n",
    "This could be useful in determining which LoRAs to merge. If we run this analysis on all of the weight matrices of two different LoRAs, then we can determine how much layer `n` of `lora1` overlaps with layer `n` of `lora2`. If the overlap is small, then the two weight martices $\\Delta W_1^{(n)} = B_1^{(n)}A_1^{(n)}$ and $\\Delta W_2^{(n)} = B_2^{(n)}A_2^{(n)}$ may express very different things because the subspaces that they span do not overlap very much. So, to be more explicit, we compute\n",
    "\n",
    "$$\n",
    "\\phi(\\Delta W_1^{(n)}, \\Delta W_2^{(n)}, i, j) = \\frac{||U_{\\Delta W_1^{(n)}}^{(i)} {U_{\\Delta W_2^{(n)}}^{(j)}}^T||_F^2}{\\min(i, j)}\n",
    "$$\n",
    "\n",
    "for a weight matrix $\\Delta W_1^{(n)}$ from the first LoRA, and the corresponding $\\Delta W_2^{(n)}$ from the second LoRA. This could indicate that merging the two LoRAs will create a more general model, able to create a wider range of diverse styles. This might also help in explaining why two LoRAs create something very muddy or undesirable when merges. Obviously, this is all conjecture based on a mathematical analysis that needs to be tested, and it does not provide a precise theshold for the overlap. What upper or lower bound might we use for this subspace similarity measure $\\phi$? Could this hypthesis be wrong, or inverted? That is, is it possible that in some cases we actually want *high* overlap between models so that we merge very similar concepts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The subspace similarity measure is: 0.26528483629226685\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# The A matrices\n",
    "A1 = lora1_tensors['lora_te_text_model_encoder_layers_0_mlp_fc1.lora_down.weight'].float()\n",
    "A2 = lora2_tensors['lora_te_text_model_encoder_layers_0_mlp_fc1.lora_down.weight'].float()\n",
    "\n",
    "# The B matrices\n",
    "B1 = lora1_tensors['lora_te_text_model_encoder_layers_0_mlp_fc1.lora_up.weight'].float()\n",
    "B2 = lora2_tensors['lora_te_text_model_encoder_layers_0_mlp_fc1.lora_up.weight'].float()\n",
    "\n",
    "# Compute the update matrices\n",
    "Delta_W1 = torch.matmul(B1, A1)\n",
    "Delta_W2 = torch.matmul(B2, A2)\n",
    "\n",
    "# Compute the SVD of the update matrices\n",
    "U1, _, _ = torch.svd(Delta_W1)\n",
    "U2, _, _ = torch.svd(Delta_W2)\n",
    "\n",
    "# Define the subspace similarity measure\n",
    "def phi(U1, U2, i, j):\n",
    "    U1_i = U1[:, :i]  # First i columns of U1\n",
    "    U2_j = U2[:, :j]  # First j columns of U2\n",
    "    \n",
    "    product = torch.matmul(U1_i.t(), U2_j)  # Matrix multiplication\n",
    "    norm = torch.norm(product)  # Frobenius norm\n",
    "    \n",
    "    return norm ** 2 / min(i, j)\n",
    "\n",
    "# Calculate the subspace similarity measure\n",
    "i = U1.size(1)  # Number of columns in U1\n",
    "j = U2.size(1)  # Number of columns in U2\n",
    "result = phi(U1, U2, i, j)  # Replace i and j with the desired values\n",
    "\n",
    "print(f'The subspace similarity measure is: {result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([3072, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([3072, 64])\n",
      "ΔW1 shape: torch.Size([3072, 768])\n",
      "ΔW2 shape: torch.Size([3072, 768])\n",
      "The subspace similarity measure for layer 0 is: 0.26528483629226685\n",
      "A1 shape: torch.Size([32, 3072])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 3072])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 3072])\n",
      "ΔW2 shape: torch.Size([768, 3072])\n",
      "The subspace similarity measure for layer 1 is: 0.9999878406524658\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 2 is: 0.9999885559082031\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 3 is: 0.9999887347221375\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 4 is: 0.9999908804893494\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 5 is: 0.9999882578849792\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([3072, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([3072, 64])\n",
      "ΔW1 shape: torch.Size([3072, 768])\n",
      "ΔW2 shape: torch.Size([3072, 768])\n",
      "The subspace similarity measure for layer 6 is: 0.26571375131607056\n",
      "A1 shape: torch.Size([32, 3072])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 3072])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 3072])\n",
      "ΔW2 shape: torch.Size([768, 3072])\n",
      "The subspace similarity measure for layer 7 is: 0.999989926815033\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 8 is: 0.9999870657920837\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 9 is: 0.9999893307685852\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 10 is: 0.9999907612800598\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 11 is: 0.9999897480010986\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([3072, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([3072, 64])\n",
      "ΔW1 shape: torch.Size([3072, 768])\n",
      "ΔW2 shape: torch.Size([3072, 768])\n",
      "The subspace similarity measure for layer 12 is: 0.28491613268852234\n",
      "A1 shape: torch.Size([32, 3072])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 3072])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 3072])\n",
      "ΔW2 shape: torch.Size([768, 3072])\n",
      "The subspace similarity measure for layer 13 is: 0.9999805092811584\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 14 is: 0.9999852180480957\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 15 is: 0.9999846816062927\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 16 is: 0.9999852180480957\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 17 is: 0.9999863505363464\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([3072, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([3072, 64])\n",
      "ΔW1 shape: torch.Size([3072, 768])\n",
      "ΔW2 shape: torch.Size([3072, 768])\n",
      "The subspace similarity measure for layer 18 is: 0.26683029532432556\n",
      "A1 shape: torch.Size([32, 3072])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 3072])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 3072])\n",
      "ΔW2 shape: torch.Size([768, 3072])\n",
      "The subspace similarity measure for layer 19 is: 0.9999917149543762\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 20 is: 0.9999911785125732\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 21 is: 0.9999918341636658\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 22 is: 0.999991238117218\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 23 is: 0.9999890923500061\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([3072, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([3072, 64])\n",
      "ΔW1 shape: torch.Size([3072, 768])\n",
      "ΔW2 shape: torch.Size([3072, 768])\n",
      "The subspace similarity measure for layer 24 is: 0.27087515592575073\n",
      "A1 shape: torch.Size([32, 3072])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 3072])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 3072])\n",
      "ΔW2 shape: torch.Size([768, 3072])\n",
      "The subspace similarity measure for layer 25 is: 0.9999900460243225\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 26 is: 0.9999902248382568\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 27 is: 0.9999883770942688\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 28 is: 0.9999927878379822\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 29 is: 0.9999895095825195\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([3072, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([3072, 64])\n",
      "ΔW1 shape: torch.Size([3072, 768])\n",
      "ΔW2 shape: torch.Size([3072, 768])\n",
      "The subspace similarity measure for layer 30 is: 0.2644103169441223\n",
      "A1 shape: torch.Size([32, 3072])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 3072])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 3072])\n",
      "ΔW2 shape: torch.Size([768, 3072])\n",
      "The subspace similarity measure for layer 31 is: 0.9999893307685852\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 32 is: 0.9999906420707703\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 33 is: 0.9999896883964539\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 34 is: 0.9999893307685852\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 35 is: 0.9999897480010986\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([3072, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([3072, 64])\n",
      "ΔW1 shape: torch.Size([3072, 768])\n",
      "ΔW2 shape: torch.Size([3072, 768])\n",
      "The subspace similarity measure for layer 36 is: 0.2637903094291687\n",
      "A1 shape: torch.Size([32, 3072])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 3072])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 3072])\n",
      "ΔW2 shape: torch.Size([768, 3072])\n",
      "The subspace similarity measure for layer 37 is: 0.9999907612800598\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 38 is: 0.9999885559082031\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 39 is: 0.9999929070472717\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 40 is: 0.9999893307685852\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 41 is: 0.9999902248382568\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([3072, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([3072, 64])\n",
      "ΔW1 shape: torch.Size([3072, 768])\n",
      "ΔW2 shape: torch.Size([3072, 768])\n",
      "The subspace similarity measure for layer 42 is: 0.2647744119167328\n",
      "A1 shape: torch.Size([32, 3072])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 3072])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 3072])\n",
      "ΔW2 shape: torch.Size([768, 3072])\n",
      "The subspace similarity measure for layer 43 is: 0.9999902248382568\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 44 is: 0.9999925494194031\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 45 is: 0.9999881386756897\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 46 is: 0.9999892115592957\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 47 is: 0.9999880194664001\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([3072, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([3072, 64])\n",
      "ΔW1 shape: torch.Size([3072, 768])\n",
      "ΔW2 shape: torch.Size([3072, 768])\n",
      "The subspace similarity measure for layer 48 is: 0.26757362484931946\n",
      "A1 shape: torch.Size([32, 3072])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 3072])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 3072])\n",
      "ΔW2 shape: torch.Size([768, 3072])\n",
      "The subspace similarity measure for layer 49 is: 0.9999914169311523\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 50 is: 0.9999897480010986\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 51 is: 0.9999881386756897\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 52 is: 0.9999889731407166\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 53 is: 0.9999887943267822\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([3072, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([3072, 64])\n",
      "ΔW1 shape: torch.Size([3072, 768])\n",
      "ΔW2 shape: torch.Size([3072, 768])\n",
      "The subspace similarity measure for layer 54 is: 0.26692089438438416\n",
      "A1 shape: torch.Size([32, 3072])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 3072])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 3072])\n",
      "ΔW2 shape: torch.Size([768, 3072])\n",
      "The subspace similarity measure for layer 55 is: 0.9999887347221375\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 56 is: 0.9999871253967285\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 57 is: 0.9999900460243225\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 58 is: 0.9999880194664001\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 59 is: 0.9999917149543762\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([3072, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([3072, 64])\n",
      "ΔW1 shape: torch.Size([3072, 768])\n",
      "ΔW2 shape: torch.Size([3072, 768])\n",
      "The subspace similarity measure for layer 60 is: 0.26369643211364746\n",
      "A1 shape: torch.Size([32, 3072])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 3072])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 3072])\n",
      "ΔW2 shape: torch.Size([768, 3072])\n",
      "The subspace similarity measure for layer 61 is: 0.9999907612800598\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 62 is: 0.9999887943267822\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 63 is: 0.9999917149543762\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 64 is: 0.9999876618385315\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 65 is: 0.9999878406524658\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([3072, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([3072, 64])\n",
      "ΔW1 shape: torch.Size([3072, 768])\n",
      "ΔW2 shape: torch.Size([3072, 768])\n",
      "The subspace similarity measure for layer 66 is: 0.2677040100097656\n",
      "A1 shape: torch.Size([32, 3072])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 3072])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 3072])\n",
      "ΔW2 shape: torch.Size([768, 3072])\n",
      "The subspace similarity measure for layer 67 is: 0.9999887943267822\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 68 is: 0.9999882578849792\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 69 is: 0.9999876022338867\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 70 is: 0.9999895095825195\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([768, 64])\n",
      "ΔW1 shape: torch.Size([768, 768])\n",
      "ΔW2 shape: torch.Size([768, 768])\n",
      "The subspace similarity measure for layer 71 is: 0.9999914169311523\n",
      "A1 shape: torch.Size([32, 320, 1, 1])\n",
      "B1 shape: torch.Size([320, 32, 1, 1])\n",
      "A2 shape: torch.Size([64, 320, 1, 1])\n",
      "B2 shape: torch.Size([320, 64, 1, 1])\n",
      "Error occurred at layer 72: The size of tensor a (32) must match the size of tensor b (320) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 320, 1, 1])\n",
      "B1 shape: torch.Size([320, 32, 1, 1])\n",
      "A2 shape: torch.Size([64, 320, 1, 1])\n",
      "B2 shape: torch.Size([320, 64, 1, 1])\n",
      "Error occurred at layer 73: The size of tensor a (32) must match the size of tensor b (320) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 320])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 320])\n",
      "ΔW2 shape: torch.Size([320, 320])\n",
      "The subspace similarity measure for layer 74 is: 0.9999980926513672\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 320])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 320])\n",
      "ΔW2 shape: torch.Size([320, 320])\n",
      "The subspace similarity measure for layer 75 is: 0.9999991655349731\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 320])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 320])\n",
      "ΔW2 shape: torch.Size([320, 320])\n",
      "The subspace similarity measure for layer 76 is: 1.0\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 320])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 320])\n",
      "ΔW2 shape: torch.Size([320, 320])\n",
      "The subspace similarity measure for layer 77 is: 0.9999987483024597\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 768])\n",
      "ΔW2 shape: torch.Size([320, 768])\n",
      "The subspace similarity measure for layer 78 is: 0.9999991655349731\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 320])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 320])\n",
      "ΔW2 shape: torch.Size([320, 320])\n",
      "The subspace similarity measure for layer 79 is: 0.9999991655349731\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 320])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 320])\n",
      "ΔW2 shape: torch.Size([320, 320])\n",
      "The subspace similarity measure for layer 80 is: 1.0000004768371582\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 768])\n",
      "ΔW2 shape: torch.Size([320, 768])\n",
      "The subspace similarity measure for layer 81 is: 0.9999998211860657\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([2560, 32])\n",
      "A2 shape: torch.Size([64, 320])\n",
      "B2 shape: torch.Size([2560, 64])\n",
      "ΔW1 shape: torch.Size([2560, 320])\n",
      "ΔW2 shape: torch.Size([2560, 320])\n",
      "The subspace similarity measure for layer 82 is: 0.17305810749530792\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 1280])\n",
      "ΔW2 shape: torch.Size([320, 1280])\n",
      "The subspace similarity measure for layer 83 is: 1.0\n",
      "A1 shape: torch.Size([32, 320, 1, 1])\n",
      "B1 shape: torch.Size([320, 32, 1, 1])\n",
      "A2 shape: torch.Size([64, 320, 1, 1])\n",
      "B2 shape: torch.Size([320, 64, 1, 1])\n",
      "Error occurred at layer 84: The size of tensor a (32) must match the size of tensor b (320) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 320, 1, 1])\n",
      "B1 shape: torch.Size([320, 32, 1, 1])\n",
      "A2 shape: torch.Size([64, 320, 1, 1])\n",
      "B2 shape: torch.Size([320, 64, 1, 1])\n",
      "Error occurred at layer 85: The size of tensor a (32) must match the size of tensor b (320) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 320])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 320])\n",
      "ΔW2 shape: torch.Size([320, 320])\n",
      "The subspace similarity measure for layer 86 is: 0.9999994039535522\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 320])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 320])\n",
      "ΔW2 shape: torch.Size([320, 320])\n",
      "The subspace similarity measure for layer 87 is: 0.999998927116394\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 320])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 320])\n",
      "ΔW2 shape: torch.Size([320, 320])\n",
      "The subspace similarity measure for layer 88 is: 0.9999991655349731\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 320])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 320])\n",
      "ΔW2 shape: torch.Size([320, 320])\n",
      "The subspace similarity measure for layer 89 is: 0.9999998211860657\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 768])\n",
      "ΔW2 shape: torch.Size([320, 768])\n",
      "The subspace similarity measure for layer 90 is: 0.9999987483024597\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 320])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 320])\n",
      "ΔW2 shape: torch.Size([320, 320])\n",
      "The subspace similarity measure for layer 91 is: 0.9999996423721313\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 320])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 320])\n",
      "ΔW2 shape: torch.Size([320, 320])\n",
      "The subspace similarity measure for layer 92 is: 0.9999985694885254\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 768])\n",
      "ΔW2 shape: torch.Size([320, 768])\n",
      "The subspace similarity measure for layer 93 is: 0.9999996423721313\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([2560, 32])\n",
      "A2 shape: torch.Size([64, 320])\n",
      "B2 shape: torch.Size([2560, 64])\n",
      "ΔW1 shape: torch.Size([2560, 320])\n",
      "ΔW2 shape: torch.Size([2560, 320])\n",
      "The subspace similarity measure for layer 94 is: 0.1714164912700653\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 1280])\n",
      "ΔW2 shape: torch.Size([320, 1280])\n",
      "The subspace similarity measure for layer 95 is: 0.999998927116394\n",
      "A1 shape: torch.Size([32, 640, 1, 1])\n",
      "B1 shape: torch.Size([640, 32, 1, 1])\n",
      "A2 shape: torch.Size([64, 640, 1, 1])\n",
      "B2 shape: torch.Size([640, 64, 1, 1])\n",
      "Error occurred at layer 96: The size of tensor a (32) must match the size of tensor b (640) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 640, 1, 1])\n",
      "B1 shape: torch.Size([640, 32, 1, 1])\n",
      "A2 shape: torch.Size([64, 640, 1, 1])\n",
      "B2 shape: torch.Size([640, 64, 1, 1])\n",
      "Error occurred at layer 97: The size of tensor a (32) must match the size of tensor b (640) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 640])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 640])\n",
      "ΔW2 shape: torch.Size([640, 640])\n",
      "The subspace similarity measure for layer 98 is: 0.9999935030937195\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 640])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 640])\n",
      "ΔW2 shape: torch.Size([640, 640])\n",
      "The subspace similarity measure for layer 99 is: 0.9999909400939941\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 640])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 640])\n",
      "ΔW2 shape: torch.Size([640, 640])\n",
      "The subspace similarity measure for layer 100 is: 0.9999929666519165\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 640])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 640])\n",
      "ΔW2 shape: torch.Size([640, 640])\n",
      "The subspace similarity measure for layer 101 is: 0.9999938011169434\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 768])\n",
      "ΔW2 shape: torch.Size([640, 768])\n",
      "The subspace similarity measure for layer 102 is: 0.9999944567680359\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 640])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 640])\n",
      "ΔW2 shape: torch.Size([640, 640])\n",
      "The subspace similarity measure for layer 103 is: 0.9999935030937195\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 640])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 640])\n",
      "ΔW2 shape: torch.Size([640, 640])\n",
      "The subspace similarity measure for layer 104 is: 0.999994158744812\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 768])\n",
      "ΔW2 shape: torch.Size([640, 768])\n",
      "The subspace similarity measure for layer 105 is: 0.999994158744812\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([5120, 32])\n",
      "A2 shape: torch.Size([64, 640])\n",
      "B2 shape: torch.Size([5120, 64])\n",
      "ΔW1 shape: torch.Size([5120, 640])\n",
      "ΔW2 shape: torch.Size([5120, 640])\n",
      "The subspace similarity measure for layer 106 is: 0.1543945074081421\n",
      "A1 shape: torch.Size([32, 2560])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 2560])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 2560])\n",
      "ΔW2 shape: torch.Size([640, 2560])\n",
      "The subspace similarity measure for layer 107 is: 0.9999948740005493\n",
      "A1 shape: torch.Size([32, 640, 1, 1])\n",
      "B1 shape: torch.Size([640, 32, 1, 1])\n",
      "A2 shape: torch.Size([64, 640, 1, 1])\n",
      "B2 shape: torch.Size([640, 64, 1, 1])\n",
      "Error occurred at layer 108: The size of tensor a (32) must match the size of tensor b (640) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 640, 1, 1])\n",
      "B1 shape: torch.Size([640, 32, 1, 1])\n",
      "A2 shape: torch.Size([64, 640, 1, 1])\n",
      "B2 shape: torch.Size([640, 64, 1, 1])\n",
      "Error occurred at layer 109: The size of tensor a (32) must match the size of tensor b (640) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 640])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 640])\n",
      "ΔW2 shape: torch.Size([640, 640])\n",
      "The subspace similarity measure for layer 110 is: 0.9999915957450867\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 640])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 640])\n",
      "ΔW2 shape: torch.Size([640, 640])\n",
      "The subspace similarity measure for layer 111 is: 0.999994158744812\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 640])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 640])\n",
      "ΔW2 shape: torch.Size([640, 640])\n",
      "The subspace similarity measure for layer 112 is: 0.9999936819076538\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 640])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 640])\n",
      "ΔW2 shape: torch.Size([640, 640])\n",
      "The subspace similarity measure for layer 113 is: 0.9999948740005493\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 768])\n",
      "ΔW2 shape: torch.Size([640, 768])\n",
      "The subspace similarity measure for layer 114 is: 0.9999935030937195\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 640])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 640])\n",
      "ΔW2 shape: torch.Size([640, 640])\n",
      "The subspace similarity measure for layer 115 is: 0.9999935030937195\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 640])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 640])\n",
      "ΔW2 shape: torch.Size([640, 640])\n",
      "The subspace similarity measure for layer 116 is: 0.9999924898147583\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 768])\n",
      "ΔW2 shape: torch.Size([640, 768])\n",
      "The subspace similarity measure for layer 117 is: 0.9999963641166687\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([5120, 32])\n",
      "A2 shape: torch.Size([64, 640])\n",
      "B2 shape: torch.Size([5120, 64])\n",
      "ΔW1 shape: torch.Size([5120, 640])\n",
      "ΔW2 shape: torch.Size([5120, 640])\n",
      "The subspace similarity measure for layer 118 is: 0.15738140046596527\n",
      "A1 shape: torch.Size([32, 2560])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 2560])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 2560])\n",
      "ΔW2 shape: torch.Size([640, 2560])\n",
      "The subspace similarity measure for layer 119 is: 0.9999927282333374\n",
      "A1 shape: torch.Size([32, 1280, 1, 1])\n",
      "B1 shape: torch.Size([1280, 32, 1, 1])\n",
      "A2 shape: torch.Size([64, 1280, 1, 1])\n",
      "B2 shape: torch.Size([1280, 64, 1, 1])\n",
      "Error occurred at layer 120: The size of tensor a (32) must match the size of tensor b (1280) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 1280, 1, 1])\n",
      "B1 shape: torch.Size([1280, 32, 1, 1])\n",
      "A2 shape: torch.Size([64, 1280, 1, 1])\n",
      "B2 shape: torch.Size([1280, 64, 1, 1])\n",
      "Error occurred at layer 121: The size of tensor a (32) must match the size of tensor b (1280) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 1280])\n",
      "ΔW2 shape: torch.Size([1280, 1280])\n",
      "The subspace similarity measure for layer 122 is: 0.9999516606330872\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 1280])\n",
      "ΔW2 shape: torch.Size([1280, 1280])\n",
      "The subspace similarity measure for layer 123 is: 0.9999529123306274\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 1280])\n",
      "ΔW2 shape: torch.Size([1280, 1280])\n",
      "The subspace similarity measure for layer 124 is: 0.9999490976333618\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 1280])\n",
      "ΔW2 shape: torch.Size([1280, 1280])\n",
      "The subspace similarity measure for layer 125 is: 0.9999464750289917\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 768])\n",
      "ΔW2 shape: torch.Size([1280, 768])\n",
      "The subspace similarity measure for layer 126 is: 0.6105306148529053\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 1280])\n",
      "ΔW2 shape: torch.Size([1280, 1280])\n",
      "The subspace similarity measure for layer 127 is: 0.9999467134475708\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 1280])\n",
      "ΔW2 shape: torch.Size([1280, 1280])\n",
      "The subspace similarity measure for layer 128 is: 0.9999494552612305\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 768])\n",
      "ΔW2 shape: torch.Size([1280, 768])\n",
      "The subspace similarity measure for layer 129 is: 0.6055203080177307\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([10240, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([10240, 64])\n",
      "ΔW1 shape: torch.Size([10240, 1280])\n",
      "ΔW2 shape: torch.Size([10240, 1280])\n",
      "The subspace similarity measure for layer 130 is: 0.1365688443183899\n",
      "A1 shape: torch.Size([32, 5120])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 5120])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 5120])\n",
      "ΔW2 shape: torch.Size([1280, 5120])\n",
      "The subspace similarity measure for layer 131 is: 0.9999482035636902\n",
      "A1 shape: torch.Size([32, 1280, 1, 1])\n",
      "B1 shape: torch.Size([1280, 32, 1, 1])\n",
      "A2 shape: torch.Size([64, 1280, 1, 1])\n",
      "B2 shape: torch.Size([1280, 64, 1, 1])\n",
      "Error occurred at layer 132: The size of tensor a (32) must match the size of tensor b (1280) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 1280, 1, 1])\n",
      "B1 shape: torch.Size([1280, 32, 1, 1])\n",
      "A2 shape: torch.Size([64, 1280, 1, 1])\n",
      "B2 shape: torch.Size([1280, 64, 1, 1])\n",
      "Error occurred at layer 133: The size of tensor a (32) must match the size of tensor b (1280) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 1280])\n",
      "ΔW2 shape: torch.Size([1280, 1280])\n",
      "The subspace similarity measure for layer 134 is: 0.99995356798172\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 1280])\n",
      "ΔW2 shape: torch.Size([1280, 1280])\n",
      "The subspace similarity measure for layer 135 is: 0.9999462962150574\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 1280])\n",
      "ΔW2 shape: torch.Size([1280, 1280])\n",
      "The subspace similarity measure for layer 136 is: 0.9999518394470215\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 1280])\n",
      "ΔW2 shape: torch.Size([1280, 1280])\n",
      "The subspace similarity measure for layer 137 is: 0.999947726726532\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 768])\n",
      "ΔW2 shape: torch.Size([1280, 768])\n",
      "The subspace similarity measure for layer 138 is: 0.6102796196937561\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 1280])\n",
      "ΔW2 shape: torch.Size([1280, 1280])\n",
      "The subspace similarity measure for layer 139 is: 0.9999510049819946\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 1280])\n",
      "ΔW2 shape: torch.Size([1280, 1280])\n",
      "The subspace similarity measure for layer 140 is: 0.999951183795929\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 768])\n",
      "ΔW2 shape: torch.Size([1280, 768])\n",
      "The subspace similarity measure for layer 141 is: 0.6098781228065491\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([10240, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([10240, 64])\n",
      "ΔW1 shape: torch.Size([10240, 1280])\n",
      "ΔW2 shape: torch.Size([10240, 1280])\n",
      "The subspace similarity measure for layer 142 is: 0.13769392669200897\n",
      "A1 shape: torch.Size([32, 5120])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 5120])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 5120])\n",
      "ΔW2 shape: torch.Size([1280, 5120])\n",
      "The subspace similarity measure for layer 143 is: 0.9999533891677856\n",
      "A1 shape: torch.Size([32, 1280, 1, 1])\n",
      "B1 shape: torch.Size([1280, 32, 1, 1])\n",
      "A2 shape: torch.Size([64, 1280, 1, 1])\n",
      "B2 shape: torch.Size([1280, 64, 1, 1])\n",
      "Error occurred at layer 144: The size of tensor a (32) must match the size of tensor b (1280) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 1280, 1, 1])\n",
      "B1 shape: torch.Size([1280, 32, 1, 1])\n",
      "A2 shape: torch.Size([64, 1280, 1, 1])\n",
      "B2 shape: torch.Size([1280, 64, 1, 1])\n",
      "Error occurred at layer 145: The size of tensor a (32) must match the size of tensor b (1280) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 1280])\n",
      "ΔW2 shape: torch.Size([1280, 1280])\n",
      "The subspace similarity measure for layer 146 is: 0.9999492764472961\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 1280])\n",
      "ΔW2 shape: torch.Size([1280, 1280])\n",
      "The subspace similarity measure for layer 147 is: 0.9999454617500305\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 1280])\n",
      "ΔW2 shape: torch.Size([1280, 1280])\n",
      "The subspace similarity measure for layer 148 is: 0.9999492764472961\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 1280])\n",
      "ΔW2 shape: torch.Size([1280, 1280])\n",
      "The subspace similarity measure for layer 149 is: 0.999952495098114\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 768])\n",
      "ΔW2 shape: torch.Size([1280, 768])\n",
      "The subspace similarity measure for layer 150 is: 0.6096875667572021\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 1280])\n",
      "ΔW2 shape: torch.Size([1280, 1280])\n",
      "The subspace similarity measure for layer 151 is: 0.999946117401123\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 1280])\n",
      "ΔW2 shape: torch.Size([1280, 1280])\n",
      "The subspace similarity measure for layer 152 is: 0.9999537467956543\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 768])\n",
      "ΔW2 shape: torch.Size([1280, 768])\n",
      "The subspace similarity measure for layer 153 is: 0.6064993143081665\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([10240, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([10240, 64])\n",
      "ΔW1 shape: torch.Size([10240, 1280])\n",
      "ΔW2 shape: torch.Size([10240, 1280])\n",
      "The subspace similarity measure for layer 154 is: 0.1332767903804779\n",
      "A1 shape: torch.Size([32, 5120])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 5120])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 5120])\n",
      "ΔW2 shape: torch.Size([1280, 5120])\n",
      "The subspace similarity measure for layer 155 is: 0.9999533891677856\n",
      "A1 shape: torch.Size([32, 1280, 1, 1])\n",
      "B1 shape: torch.Size([1280, 32, 1, 1])\n",
      "A2 shape: torch.Size([64, 1280, 1, 1])\n",
      "B2 shape: torch.Size([1280, 64, 1, 1])\n",
      "Error occurred at layer 156: The size of tensor a (32) must match the size of tensor b (1280) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 1280, 1, 1])\n",
      "B1 shape: torch.Size([1280, 32, 1, 1])\n",
      "A2 shape: torch.Size([64, 1280, 1, 1])\n",
      "B2 shape: torch.Size([1280, 64, 1, 1])\n",
      "Error occurred at layer 157: The size of tensor a (32) must match the size of tensor b (1280) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 1280])\n",
      "ΔW2 shape: torch.Size([1280, 1280])\n",
      "The subspace similarity measure for layer 158 is: 0.9999464750289917\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 1280])\n",
      "ΔW2 shape: torch.Size([1280, 1280])\n",
      "The subspace similarity measure for layer 159 is: 0.9999462962150574\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 1280])\n",
      "ΔW2 shape: torch.Size([1280, 1280])\n",
      "The subspace similarity measure for layer 160 is: 0.9999482035636902\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 1280])\n",
      "ΔW2 shape: torch.Size([1280, 1280])\n",
      "The subspace similarity measure for layer 161 is: 0.9999533891677856\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 768])\n",
      "ΔW2 shape: torch.Size([1280, 768])\n",
      "The subspace similarity measure for layer 162 is: 0.607451856136322\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 1280])\n",
      "ΔW2 shape: torch.Size([1280, 1280])\n",
      "The subspace similarity measure for layer 163 is: 0.9999510049819946\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 1280])\n",
      "ΔW2 shape: torch.Size([1280, 1280])\n",
      "The subspace similarity measure for layer 164 is: 0.9999456405639648\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 768])\n",
      "ΔW2 shape: torch.Size([1280, 768])\n",
      "The subspace similarity measure for layer 165 is: 0.6084207892417908\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([10240, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([10240, 64])\n",
      "ΔW1 shape: torch.Size([10240, 1280])\n",
      "ΔW2 shape: torch.Size([10240, 1280])\n",
      "The subspace similarity measure for layer 166 is: 0.13885697722434998\n",
      "A1 shape: torch.Size([32, 5120])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 5120])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 5120])\n",
      "ΔW2 shape: torch.Size([1280, 5120])\n",
      "The subspace similarity measure for layer 167 is: 0.9999513626098633\n",
      "A1 shape: torch.Size([32, 1280, 1, 1])\n",
      "B1 shape: torch.Size([1280, 32, 1, 1])\n",
      "A2 shape: torch.Size([64, 1280, 1, 1])\n",
      "B2 shape: torch.Size([1280, 64, 1, 1])\n",
      "Error occurred at layer 168: The size of tensor a (32) must match the size of tensor b (1280) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 1280, 1, 1])\n",
      "B1 shape: torch.Size([1280, 32, 1, 1])\n",
      "A2 shape: torch.Size([64, 1280, 1, 1])\n",
      "B2 shape: torch.Size([1280, 64, 1, 1])\n",
      "Error occurred at layer 169: The size of tensor a (32) must match the size of tensor b (1280) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 1280])\n",
      "ΔW2 shape: torch.Size([1280, 1280])\n",
      "The subspace similarity measure for layer 170 is: 0.9999482035636902\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 1280])\n",
      "ΔW2 shape: torch.Size([1280, 1280])\n",
      "The subspace similarity measure for layer 171 is: 0.9999467134475708\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 1280])\n",
      "ΔW2 shape: torch.Size([1280, 1280])\n",
      "The subspace similarity measure for layer 172 is: 0.9999497532844543\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 1280])\n",
      "ΔW2 shape: torch.Size([1280, 1280])\n",
      "The subspace similarity measure for layer 173 is: 0.9999483823776245\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 768])\n",
      "ΔW2 shape: torch.Size([1280, 768])\n",
      "The subspace similarity measure for layer 174 is: 0.6065221428871155\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 1280])\n",
      "ΔW2 shape: torch.Size([1280, 1280])\n",
      "The subspace similarity measure for layer 175 is: 0.999950110912323\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 1280])\n",
      "ΔW2 shape: torch.Size([1280, 1280])\n",
      "The subspace similarity measure for layer 176 is: 0.999952495098114\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 768])\n",
      "ΔW2 shape: torch.Size([1280, 768])\n",
      "The subspace similarity measure for layer 177 is: 0.6052832007408142\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([10240, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([10240, 64])\n",
      "ΔW1 shape: torch.Size([10240, 1280])\n",
      "ΔW2 shape: torch.Size([10240, 1280])\n",
      "The subspace similarity measure for layer 178 is: 0.13806910812854767\n",
      "A1 shape: torch.Size([32, 5120])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 5120])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 5120])\n",
      "ΔW2 shape: torch.Size([1280, 5120])\n",
      "The subspace similarity measure for layer 179 is: 0.9999533891677856\n",
      "A1 shape: torch.Size([32, 1280, 1, 1])\n",
      "B1 shape: torch.Size([1280, 32, 1, 1])\n",
      "A2 shape: torch.Size([64, 1280, 1, 1])\n",
      "B2 shape: torch.Size([1280, 64, 1, 1])\n",
      "Error occurred at layer 180: The size of tensor a (32) must match the size of tensor b (1280) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 1280, 1, 1])\n",
      "B1 shape: torch.Size([1280, 32, 1, 1])\n",
      "A2 shape: torch.Size([64, 1280, 1, 1])\n",
      "B2 shape: torch.Size([1280, 64, 1, 1])\n",
      "Error occurred at layer 181: The size of tensor a (32) must match the size of tensor b (1280) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 1280])\n",
      "ΔW2 shape: torch.Size([1280, 1280])\n",
      "The subspace similarity measure for layer 182 is: 0.9999497532844543\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 1280])\n",
      "ΔW2 shape: torch.Size([1280, 1280])\n",
      "The subspace similarity measure for layer 183 is: 0.999947726726532\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 1280])\n",
      "ΔW2 shape: torch.Size([1280, 1280])\n",
      "The subspace similarity measure for layer 184 is: 0.9999490976333618\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 1280])\n",
      "ΔW2 shape: torch.Size([1280, 1280])\n",
      "The subspace similarity measure for layer 185 is: 0.9999494552612305\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 768])\n",
      "ΔW2 shape: torch.Size([1280, 768])\n",
      "The subspace similarity measure for layer 186 is: 0.6104145646095276\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 1280])\n",
      "ΔW2 shape: torch.Size([1280, 1280])\n",
      "The subspace similarity measure for layer 187 is: 0.9999492764472961\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 1280])\n",
      "ΔW2 shape: torch.Size([1280, 1280])\n",
      "The subspace similarity measure for layer 188 is: 0.9999518394470215\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 768])\n",
      "ΔW2 shape: torch.Size([1280, 768])\n",
      "The subspace similarity measure for layer 189 is: 0.6065794229507446\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([10240, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([10240, 64])\n",
      "ΔW1 shape: torch.Size([10240, 1280])\n",
      "ΔW2 shape: torch.Size([10240, 1280])\n",
      "The subspace similarity measure for layer 190 is: 0.13888858258724213\n",
      "A1 shape: torch.Size([32, 5120])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "A2 shape: torch.Size([64, 5120])\n",
      "B2 shape: torch.Size([1280, 64])\n",
      "ΔW1 shape: torch.Size([1280, 5120])\n",
      "ΔW2 shape: torch.Size([1280, 5120])\n",
      "The subspace similarity measure for layer 191 is: 0.9999510049819946\n",
      "A1 shape: torch.Size([32, 640, 1, 1])\n",
      "B1 shape: torch.Size([640, 32, 1, 1])\n",
      "A2 shape: torch.Size([64, 640, 1, 1])\n",
      "B2 shape: torch.Size([640, 64, 1, 1])\n",
      "Error occurred at layer 192: The size of tensor a (32) must match the size of tensor b (640) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 640, 1, 1])\n",
      "B1 shape: torch.Size([640, 32, 1, 1])\n",
      "A2 shape: torch.Size([64, 640, 1, 1])\n",
      "B2 shape: torch.Size([640, 64, 1, 1])\n",
      "Error occurred at layer 193: The size of tensor a (32) must match the size of tensor b (640) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 640])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 640])\n",
      "ΔW2 shape: torch.Size([640, 640])\n",
      "The subspace similarity measure for layer 194 is: 0.999995231628418\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 640])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 640])\n",
      "ΔW2 shape: torch.Size([640, 640])\n",
      "The subspace similarity measure for layer 195 is: 0.9999944567680359\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 640])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 640])\n",
      "ΔW2 shape: torch.Size([640, 640])\n",
      "The subspace similarity measure for layer 196 is: 0.9999953508377075\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 640])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 640])\n",
      "ΔW2 shape: torch.Size([640, 640])\n",
      "The subspace similarity measure for layer 197 is: 0.9999927282333374\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 768])\n",
      "ΔW2 shape: torch.Size([640, 768])\n",
      "The subspace similarity measure for layer 198 is: 0.999996542930603\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 640])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 640])\n",
      "ΔW2 shape: torch.Size([640, 640])\n",
      "The subspace similarity measure for layer 199 is: 0.9999920129776001\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 640])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 640])\n",
      "ΔW2 shape: torch.Size([640, 640])\n",
      "The subspace similarity measure for layer 200 is: 0.9999912977218628\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 768])\n",
      "ΔW2 shape: torch.Size([640, 768])\n",
      "The subspace similarity measure for layer 201 is: 0.9999947547912598\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([5120, 32])\n",
      "A2 shape: torch.Size([64, 640])\n",
      "B2 shape: torch.Size([5120, 64])\n",
      "ΔW1 shape: torch.Size([5120, 640])\n",
      "ΔW2 shape: torch.Size([5120, 640])\n",
      "The subspace similarity measure for layer 202 is: 0.16359271109104156\n",
      "A1 shape: torch.Size([32, 2560])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 2560])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 2560])\n",
      "ΔW2 shape: torch.Size([640, 2560])\n",
      "The subspace similarity measure for layer 203 is: 0.9999944567680359\n",
      "A1 shape: torch.Size([32, 640, 1, 1])\n",
      "B1 shape: torch.Size([640, 32, 1, 1])\n",
      "A2 shape: torch.Size([64, 640, 1, 1])\n",
      "B2 shape: torch.Size([640, 64, 1, 1])\n",
      "Error occurred at layer 204: The size of tensor a (32) must match the size of tensor b (640) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 640, 1, 1])\n",
      "B1 shape: torch.Size([640, 32, 1, 1])\n",
      "A2 shape: torch.Size([64, 640, 1, 1])\n",
      "B2 shape: torch.Size([640, 64, 1, 1])\n",
      "Error occurred at layer 205: The size of tensor a (32) must match the size of tensor b (640) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 640])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 640])\n",
      "ΔW2 shape: torch.Size([640, 640])\n",
      "The subspace similarity measure for layer 206 is: 0.9999931454658508\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 640])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 640])\n",
      "ΔW2 shape: torch.Size([640, 640])\n",
      "The subspace similarity measure for layer 207 is: 0.9999938011169434\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 640])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 640])\n",
      "ΔW2 shape: torch.Size([640, 640])\n",
      "The subspace similarity measure for layer 208 is: 0.9999936819076538\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 640])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 640])\n",
      "ΔW2 shape: torch.Size([640, 640])\n",
      "The subspace similarity measure for layer 209 is: 0.9999963045120239\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 768])\n",
      "ΔW2 shape: torch.Size([640, 768])\n",
      "The subspace similarity measure for layer 210 is: 0.9999942779541016\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 640])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 640])\n",
      "ΔW2 shape: torch.Size([640, 640])\n",
      "The subspace similarity measure for layer 211 is: 0.9999939799308777\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 640])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 640])\n",
      "ΔW2 shape: torch.Size([640, 640])\n",
      "The subspace similarity measure for layer 212 is: 0.9999924898147583\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 768])\n",
      "ΔW2 shape: torch.Size([640, 768])\n",
      "The subspace similarity measure for layer 213 is: 0.9999938011169434\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([5120, 32])\n",
      "A2 shape: torch.Size([64, 640])\n",
      "B2 shape: torch.Size([5120, 64])\n",
      "ΔW1 shape: torch.Size([5120, 640])\n",
      "ΔW2 shape: torch.Size([5120, 640])\n",
      "The subspace similarity measure for layer 214 is: 0.1611660271883011\n",
      "A1 shape: torch.Size([32, 2560])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 2560])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 2560])\n",
      "ΔW2 shape: torch.Size([640, 2560])\n",
      "The subspace similarity measure for layer 215 is: 0.999994158744812\n",
      "A1 shape: torch.Size([32, 640, 1, 1])\n",
      "B1 shape: torch.Size([640, 32, 1, 1])\n",
      "A2 shape: torch.Size([64, 640, 1, 1])\n",
      "B2 shape: torch.Size([640, 64, 1, 1])\n",
      "Error occurred at layer 216: The size of tensor a (32) must match the size of tensor b (640) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 640, 1, 1])\n",
      "B1 shape: torch.Size([640, 32, 1, 1])\n",
      "A2 shape: torch.Size([64, 640, 1, 1])\n",
      "B2 shape: torch.Size([640, 64, 1, 1])\n",
      "Error occurred at layer 217: The size of tensor a (32) must match the size of tensor b (640) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 640])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 640])\n",
      "ΔW2 shape: torch.Size([640, 640])\n",
      "The subspace similarity measure for layer 218 is: 0.9999934434890747\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 640])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 640])\n",
      "ΔW2 shape: torch.Size([640, 640])\n",
      "The subspace similarity measure for layer 219 is: 0.9999917149543762\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 640])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 640])\n",
      "ΔW2 shape: torch.Size([640, 640])\n",
      "The subspace similarity measure for layer 220 is: 0.9999934434890747\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 640])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 640])\n",
      "ΔW2 shape: torch.Size([640, 640])\n",
      "The subspace similarity measure for layer 221 is: 0.9999942779541016\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 768])\n",
      "ΔW2 shape: torch.Size([640, 768])\n",
      "The subspace similarity measure for layer 222 is: 0.9999932050704956\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 640])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 640])\n",
      "ΔW2 shape: torch.Size([640, 640])\n",
      "The subspace similarity measure for layer 223 is: 0.9999909400939941\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 640])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 640])\n",
      "ΔW2 shape: torch.Size([640, 640])\n",
      "The subspace similarity measure for layer 224 is: 0.999995231628418\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 768])\n",
      "ΔW2 shape: torch.Size([640, 768])\n",
      "The subspace similarity measure for layer 225 is: 0.9999920129776001\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([5120, 32])\n",
      "A2 shape: torch.Size([64, 640])\n",
      "B2 shape: torch.Size([5120, 64])\n",
      "ΔW1 shape: torch.Size([5120, 640])\n",
      "ΔW2 shape: torch.Size([5120, 640])\n",
      "The subspace similarity measure for layer 226 is: 0.16014735400676727\n",
      "A1 shape: torch.Size([32, 2560])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "A2 shape: torch.Size([64, 2560])\n",
      "B2 shape: torch.Size([640, 64])\n",
      "ΔW1 shape: torch.Size([640, 2560])\n",
      "ΔW2 shape: torch.Size([640, 2560])\n",
      "The subspace similarity measure for layer 227 is: 0.9999931454658508\n",
      "A1 shape: torch.Size([32, 320, 1, 1])\n",
      "B1 shape: torch.Size([320, 32, 1, 1])\n",
      "A2 shape: torch.Size([64, 320, 1, 1])\n",
      "B2 shape: torch.Size([320, 64, 1, 1])\n",
      "Error occurred at layer 228: The size of tensor a (32) must match the size of tensor b (320) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 320, 1, 1])\n",
      "B1 shape: torch.Size([320, 32, 1, 1])\n",
      "A2 shape: torch.Size([64, 320, 1, 1])\n",
      "B2 shape: torch.Size([320, 64, 1, 1])\n",
      "Error occurred at layer 229: The size of tensor a (32) must match the size of tensor b (320) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 320])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 320])\n",
      "ΔW2 shape: torch.Size([320, 320])\n",
      "The subspace similarity measure for layer 230 is: 0.9999991655349731\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 320])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 320])\n",
      "ΔW2 shape: torch.Size([320, 320])\n",
      "The subspace similarity measure for layer 231 is: 0.9999982714653015\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 320])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 320])\n",
      "ΔW2 shape: torch.Size([320, 320])\n",
      "The subspace similarity measure for layer 232 is: 0.9999987483024597\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 320])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 320])\n",
      "ΔW2 shape: torch.Size([320, 320])\n",
      "The subspace similarity measure for layer 233 is: 0.9999998211860657\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 768])\n",
      "ΔW2 shape: torch.Size([320, 768])\n",
      "The subspace similarity measure for layer 234 is: 0.9999980926513672\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 320])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 320])\n",
      "ΔW2 shape: torch.Size([320, 320])\n",
      "The subspace similarity measure for layer 235 is: 0.9999998211860657\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 320])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 320])\n",
      "ΔW2 shape: torch.Size([320, 320])\n",
      "The subspace similarity measure for layer 236 is: 1.0\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 768])\n",
      "ΔW2 shape: torch.Size([320, 768])\n",
      "The subspace similarity measure for layer 237 is: 0.9999987483024597\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([2560, 32])\n",
      "A2 shape: torch.Size([64, 320])\n",
      "B2 shape: torch.Size([2560, 64])\n",
      "ΔW1 shape: torch.Size([2560, 320])\n",
      "ΔW2 shape: torch.Size([2560, 320])\n",
      "The subspace similarity measure for layer 238 is: 0.1775144338607788\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 1280])\n",
      "ΔW2 shape: torch.Size([320, 1280])\n",
      "The subspace similarity measure for layer 239 is: 0.9999994039535522\n",
      "A1 shape: torch.Size([32, 320, 1, 1])\n",
      "B1 shape: torch.Size([320, 32, 1, 1])\n",
      "A2 shape: torch.Size([64, 320, 1, 1])\n",
      "B2 shape: torch.Size([320, 64, 1, 1])\n",
      "Error occurred at layer 240: The size of tensor a (32) must match the size of tensor b (320) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 320, 1, 1])\n",
      "B1 shape: torch.Size([320, 32, 1, 1])\n",
      "A2 shape: torch.Size([64, 320, 1, 1])\n",
      "B2 shape: torch.Size([320, 64, 1, 1])\n",
      "Error occurred at layer 241: The size of tensor a (32) must match the size of tensor b (320) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 320])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 320])\n",
      "ΔW2 shape: torch.Size([320, 320])\n",
      "The subspace similarity measure for layer 242 is: 0.9999998211860657\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 320])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 320])\n",
      "ΔW2 shape: torch.Size([320, 320])\n",
      "The subspace similarity measure for layer 243 is: 0.9999996423721313\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 320])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 320])\n",
      "ΔW2 shape: torch.Size([320, 320])\n",
      "The subspace similarity measure for layer 244 is: 0.9999994039535522\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 320])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 320])\n",
      "ΔW2 shape: torch.Size([320, 320])\n",
      "The subspace similarity measure for layer 245 is: 0.9999998211860657\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 768])\n",
      "ΔW2 shape: torch.Size([320, 768])\n",
      "The subspace similarity measure for layer 246 is: 0.9999985694885254\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 320])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 320])\n",
      "ΔW2 shape: torch.Size([320, 320])\n",
      "The subspace similarity measure for layer 247 is: 0.9999982714653015\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 320])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 320])\n",
      "ΔW2 shape: torch.Size([320, 320])\n",
      "The subspace similarity measure for layer 248 is: 0.9999998211860657\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 768])\n",
      "ΔW2 shape: torch.Size([320, 768])\n",
      "The subspace similarity measure for layer 249 is: 0.9999996423721313\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([2560, 32])\n",
      "A2 shape: torch.Size([64, 320])\n",
      "B2 shape: torch.Size([2560, 64])\n",
      "ΔW1 shape: torch.Size([2560, 320])\n",
      "ΔW2 shape: torch.Size([2560, 320])\n",
      "The subspace similarity measure for layer 250 is: 0.18077997863292694\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 1280])\n",
      "ΔW2 shape: torch.Size([320, 1280])\n",
      "The subspace similarity measure for layer 251 is: 0.999998927116394\n",
      "A1 shape: torch.Size([32, 320, 1, 1])\n",
      "B1 shape: torch.Size([320, 32, 1, 1])\n",
      "A2 shape: torch.Size([64, 320, 1, 1])\n",
      "B2 shape: torch.Size([320, 64, 1, 1])\n",
      "Error occurred at layer 252: The size of tensor a (32) must match the size of tensor b (320) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 320, 1, 1])\n",
      "B1 shape: torch.Size([320, 32, 1, 1])\n",
      "A2 shape: torch.Size([64, 320, 1, 1])\n",
      "B2 shape: torch.Size([320, 64, 1, 1])\n",
      "Error occurred at layer 253: The size of tensor a (32) must match the size of tensor b (320) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 320])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 320])\n",
      "ΔW2 shape: torch.Size([320, 320])\n",
      "The subspace similarity measure for layer 254 is: 0.9999987483024597\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 320])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 320])\n",
      "ΔW2 shape: torch.Size([320, 320])\n",
      "The subspace similarity measure for layer 255 is: 0.9999987483024597\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 320])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 320])\n",
      "ΔW2 shape: torch.Size([320, 320])\n",
      "The subspace similarity measure for layer 256 is: 1.0\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 320])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 320])\n",
      "ΔW2 shape: torch.Size([320, 320])\n",
      "The subspace similarity measure for layer 257 is: 0.9999998211860657\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 768])\n",
      "ΔW2 shape: torch.Size([320, 768])\n",
      "The subspace similarity measure for layer 258 is: 0.9999987483024597\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 320])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 320])\n",
      "ΔW2 shape: torch.Size([320, 320])\n",
      "The subspace similarity measure for layer 259 is: 0.9999987483024597\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 320])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 320])\n",
      "ΔW2 shape: torch.Size([320, 320])\n",
      "The subspace similarity measure for layer 260 is: 0.9999987483024597\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 768])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 768])\n",
      "ΔW2 shape: torch.Size([320, 768])\n",
      "The subspace similarity measure for layer 261 is: 0.999998927116394\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([2560, 32])\n",
      "A2 shape: torch.Size([64, 320])\n",
      "B2 shape: torch.Size([2560, 64])\n",
      "ΔW1 shape: torch.Size([2560, 320])\n",
      "ΔW2 shape: torch.Size([2560, 320])\n",
      "The subspace similarity measure for layer 262 is: 0.17570427060127258\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "A2 shape: torch.Size([64, 1280])\n",
      "B2 shape: torch.Size([320, 64])\n",
      "ΔW1 shape: torch.Size([320, 1280])\n",
      "ΔW2 shape: torch.Size([320, 1280])\n",
      "The subspace similarity measure for layer 263 is: 0.9999998211860657\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Gather all keys and sort them\n",
    "all_keys = sorted(list(lora1_tensors.keys()))\n",
    "\n",
    "# Filter keys for lora_down and lora_up pairs\n",
    "lora_down_keys = [key for key in all_keys if 'lora_down' in key]\n",
    "lora_up_keys = [key for key in all_keys if 'lora_up' in key]\n",
    "\n",
    "# Ensure we have matching pairs of keys\n",
    "assert len(lora_down_keys) == len(lora_up_keys), \"Mismatch in number of 'lora_down' and 'lora_up' keys\"\n",
    "\n",
    "# Define the subspace similarity measure\n",
    "def phi(U1, U2, i, j):\n",
    "    U1_i = U1[:, :i]  # First i columns of U1\n",
    "    U2_j = U2[:, :j]  # First j columns of U2\n",
    "    \n",
    "    product = torch.matmul(U1_i.t(), U2_j)  # Matrix multiplication\n",
    "    norm = torch.norm(product)  # Frobenius norm\n",
    "    \n",
    "    return norm ** 2 / min(i, j)\n",
    "\n",
    "# Iterate over all layers\n",
    "for layer in range(len(lora_down_keys)):\n",
    "    try:\n",
    "        # Extract the corresponding A and B matrices\n",
    "        A1 = lora1_tensors[lora_down_keys[layer]].float()\n",
    "        B1 = lora1_tensors[lora_up_keys[layer]].float()\n",
    "\n",
    "        A2 = lora2_tensors[lora_down_keys[layer]].float()\n",
    "        B2 = lora2_tensors[lora_up_keys[layer]].float()\n",
    "\n",
    "        # Print the shapes of A1 and B1 matrices for troubleshooting\n",
    "        print(f\"A1 shape: {A1.shape}\")\n",
    "        print(f\"B1 shape: {B1.shape}\")\n",
    "\n",
    "        print(f\"A2 shape: {A2.shape}\")\n",
    "        print(f\"B2 shape: {B2.shape}\")\n",
    "\n",
    "        # Compute the update matrices\n",
    "        Delta_W1 = torch.matmul(B1, A1)\n",
    "        print(f\"ΔW1 shape: {Delta_W1.shape}\")\n",
    "        Delta_W2 = torch.matmul(B2, A2)\n",
    "        print(f\"ΔW2 shape: {Delta_W2.shape}\")\n",
    "\n",
    "\n",
    "        # Compute the SVD of the update matrices\n",
    "        U1, _, _ = torch.svd(Delta_W1)\n",
    "        U2, _, _ = torch.svd(Delta_W2)\n",
    "\n",
    "        # Calculate the subspace similarity measure\n",
    "        i = U1.size(1)  # Number of columns in U1\n",
    "        j = U2.size(1)  # Number of columns in U2\n",
    "        result = phi(U1, U2, i, j)  # Replace i and j with the desired values\n",
    "\n",
    "        print(f\"The subspace similarity measure for layer {layer} is: {result}\")\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        # Print the layer number and the error message\n",
    "        print(f\"Error occurred at layer {layer}: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grassmannian (Fréchet/Karcher) Mean For Square Update Matrices in two LoRA Models\n",
    "\n",
    "The following is supposed to implement the Fréchet (a.k.a. Karcher) mean of two points on the Grassmannian. If we know that the matrices are square, as most of the weight matrices in Stable Diffusion are (and therefore so are the update matrices of LoRA models), then we can use it to compute the Fréchet mean of any of the square matrices for two LoRAs. In particular, we can compute the Fréchet mean of two update weight matrices $\\Delta W_1^{(n)}$ and $\\Delta W_2^{(n)}$ for some fixed layer `n` of the LoRA models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import svd\n",
    "\n",
    "def grassmann_mean(points, max_iters=100, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Computes the Fréchet mean of a set of points on the Grassmann manifold.\n",
    "\n",
    "    Parameters:\n",
    "    - points: a list of numpy arrays, each representing a point on the Grassmannian.\n",
    "    - max_iters: the maximum number of iterations for the algorithm.\n",
    "    - tol: the tolerance for convergence.\n",
    "\n",
    "    Returns:\n",
    "    - The Fréchet mean as a numpy array.\n",
    "    \"\"\"\n",
    "    # Initialize the mean to the first point\n",
    "    mean = np.linalg.qr(points[0])[0]\n",
    "    for _ in range(max_iters):\n",
    "        # Compute the tangent space at the current mean\n",
    "        tangent_space = sum([log_map(mean, np.linalg.qr(p)[0]) for p in points]) / len(points)\n",
    "        # Update the mean\n",
    "        new_mean = exp_map(mean, tangent_space)\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(new_mean - mean) < tol:\n",
    "            return new_mean\n",
    "        mean = new_mean\n",
    "    return mean\n",
    "\n",
    "def log_map(p, q):\n",
    "    \"\"\"\n",
    "    Computes the logarithm map at p of q, i.e., the tangent vector at p that points towards q.\n",
    "\n",
    "    Parameters:\n",
    "    - p: a numpy array representing a point on the Grassmannian.\n",
    "    - q: a numpy array representing a point on the Grassmannian.\n",
    "\n",
    "    Returns:\n",
    "    - The tangent vector at p that points towards q.\n",
    "    \"\"\"\n",
    "    # Compute the SVD of p^T q\n",
    "    u, s, vt = svd(np.dot(p.T, q))\n",
    "    # Clip the singular values to the range [-1, 1]\n",
    "    s = np.clip(s, -1, 1)\n",
    "    # Compute the logarithm map\n",
    "    return np.dot(p, np.dot(u, np.dot(np.diag(np.arccos(s)), vt)))\n",
    "\n",
    "\n",
    "def exp_map(p, x):\n",
    "    \"\"\"\n",
    "    Computes the exponential map at p of x, i.e., the point on the Grassmannian reached by following the geodesic in the direction of x from p.\n",
    "\n",
    "    Parameters:\n",
    "    - p: a numpy array representing a point on the Grassmannian.\n",
    "    - x: a numpy array representing a tangent vector at p.\n",
    "\n",
    "    Returns:\n",
    "    - The point on the Grassmannian reached by following the geodesic in the direction of x from p.\n",
    "    \"\"\"\n",
    "    # Compute the SVD of x\n",
    "    u, s, vt = svd(x)\n",
    "    # Compute the exponential map\n",
    "    return np.dot(p, np.dot(u, np.dot(np.diag(np.cos(s)), vt))) + np.dot(u, np.dot(np.diag(np.sin(s)), vt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.41942679  0.55888978 -0.84660875]\n",
      " [ 0.2804597  -0.98964856 -0.45203556]\n",
      " [-0.9643001   0.02577428 -0.5265871 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_random_orthogonal_matrix(dim):\n",
    "    \"\"\"\n",
    "    Generates a random orthogonal matrix of the given dimension.\n",
    "\n",
    "    Parameters:\n",
    "    - dim: the dimension of the matrix.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array representing an orthogonal matrix of the given dimension.\n",
    "    \"\"\"\n",
    "    # Generate a random matrix\n",
    "    mat = np.random.randn(dim, dim)\n",
    "    # Compute the QR decomposition of the matrix\n",
    "    q, _ = np.linalg.qr(mat)\n",
    "    return q\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate a list of points on the Grassmannian\n",
    "points = [generate_random_orthogonal_matrix(3) for _ in range(5)]\n",
    "\n",
    "# Compute the Fréchet mean of the points\n",
    "mean = grassmann_mean(points)\n",
    "\n",
    "print(mean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Another Method Using UQpy\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orthonormal matrix 1:\n",
      " [[-0.54467462 -0.25164423  0.4287992 ]\n",
      " [-0.44596758 -0.43639108 -0.66882015]\n",
      " [-0.00357031  0.71634447 -0.4014099 ]\n",
      " [-0.55902041  0.44829261  0.34847733]\n",
      " [-0.43809348  0.17922708 -0.29367412]]\n",
      "Orthonormal matrix 2:\n",
      " [[-0.14734214  0.83070799 -0.11600762]\n",
      " [-0.55904048 -0.0441168   0.80508795]\n",
      " [-0.53750149 -0.34500269 -0.29623097]\n",
      " [-0.38392218  0.40969309 -0.10137276]\n",
      " [-0.47901978 -0.14526752 -0.49025136]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate two random 3x3 matrices\n",
    "matrix1 = np.random.rand(5, 3)\n",
    "matrix2 = np.random.rand(5, 3)\n",
    "\n",
    "# Perform QR decomposition on the matrices to get orthonormal matrices\n",
    "Q1, _ = np.linalg.qr(matrix1)\n",
    "Q2, _ = np.linalg.qr(matrix2)\n",
    "\n",
    "# Ensure the matrices are of type float\n",
    "Q1 = Q1.astype(float)\n",
    "Q2 = Q2.astype(float)\n",
    "\n",
    "print(\"Orthonormal matrix 1:\\n\", Q1)\n",
    "print(\"Orthonormal matrix 2:\\n\", Q2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def is_orthonormal(matrix):\n",
    "    return np.allclose(matrix.T @ matrix, np.eye(matrix.shape[1]))\n",
    "\n",
    "print(is_orthonormal(orthonormal_matrix1))\n",
    "print(is_orthonormal(orthonormal_matrix2))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gram-Schidt from Scratch\n",
    "Just in case we need it, we can use Gram-Schmidt to produce orthonormal matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gram_schmidt(A):\n",
    "    Q = np.zeros_like(A)\n",
    "    for i in range(A.shape[1]):\n",
    "        # Start with the i'th column of A\n",
    "        q = A[:, i]\n",
    "        # Subtract projections onto the previous vectors\n",
    "        for j in range(i):\n",
    "            q -= np.dot(Q[:, j], A[:, i]) * Q[:, j]\n",
    "        # Normalize\n",
    "        Q[:, i] = q / np.linalg.norm(q)\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "orthonormal_matrix1 = gram_schmidt(matrix1)\n",
    "orthonormal_matrix2 = gram_schmidt(matrix2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Karcher Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Karcher mean of the points is: <UQpy.utilities.GrassmannPoint.GrassmannPoint object at 0x7fb50a543070>\n",
      "[[ 0.54457918  0.81385501  0.20266606]\n",
      " [ 0.41149671 -0.46982705  0.78098207]\n",
      " [ 0.73082417 -0.34191016 -0.59075669]]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary classes and methods\n",
    "from UQpy.utilities.GrassmannPoint import GrassmannPoint\n",
    "from UQpy.dimension_reduction.grassmann_manifold import GrassmannOperations\n",
    "from UQpy.utilities.distances.grassmannian_distances.AsimovDistance import AsimovDistance\n",
    "from UQpy.utilities.distances.grassmannian_distances.BinetCauchyDistance import BinetCauchyDistance\n",
    "from UQpy.utilities.distances.grassmannian_distances.FubiniStudyDistance import FubiniStudyDistance\n",
    "from UQpy.utilities.distances.grassmannian_distances.GeodesicDistance import GeodesicDistance\n",
    "from UQpy.utilities.distances.grassmannian_distances.MartinDistance import MartinDistance\n",
    "from UQpy.utilities.distances.grassmannian_distances.ProcrustesDistance import ProcrustesDistance\n",
    "from UQpy.utilities.distances.grassmannian_distances.ProjectionDistance import ProjectionDistance\n",
    "from UQpy.utilities.distances.grassmannian_distances.SpectralDistance import SpectralDistance\n",
    "\n",
    "\n",
    "# Define the points on the Grassmann manifold\n",
    "X1 = GrassmannPoint(orthonormal_matrix1)\n",
    "X2 = GrassmannPoint(orthonormal_matrix2)\n",
    "\n",
    "# List of points\n",
    "grassmann_points = [X1, X2]\n",
    "\n",
    "# Define the distance measure\n",
    "# distance = AsimovDistance()\n",
    "# distance = BinetCauchyDistance()\n",
    "# distance = FubiniStudyDistance()\n",
    "distance = GeodesicDistance()\n",
    "# distance = MartinDistance()\n",
    "# distance = ProcrustesDistance()\n",
    "# distance = ProjectionDistance()\n",
    "# distance = SpectralDistance()\n",
    "\n",
    "# Compute Karcher mean using StochasticGradientDescent\n",
    "karcher_mean = GrassmannOperations.karcher_mean(\n",
    "    grassmann_points=grassmann_points, \n",
    "    optimization_method='StochasticGradientDescent', \n",
    "    distance=distance, \n",
    "    acceleration=False, \n",
    "    tolerance=0.001, \n",
    "    maximum_iterations=1000\n",
    ")\n",
    "\n",
    "print(\"The Karcher mean of the points is:\", karcher_mean)\n",
    "print(karcher_mean.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
