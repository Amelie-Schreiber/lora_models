{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in lora1_tensors:\n",
      "lora_te_text_model_encoder_layers_0_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_0_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_0_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_0_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_0_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_0_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_0_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_0_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_0_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_0_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_0_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_0_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_0_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_0_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_0_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_0_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_0_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_0_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_10_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_10_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_10_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_10_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_10_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_10_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_10_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_10_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_10_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_10_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_10_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_10_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_10_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_10_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_10_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_10_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_10_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_10_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_11_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_11_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_11_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_11_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_11_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_11_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_11_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_11_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_11_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_11_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_11_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_11_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_11_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_11_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_11_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_11_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_11_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_11_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_1_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_1_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_1_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_1_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_1_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_1_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_1_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_1_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_1_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_1_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_1_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_1_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_1_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_1_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_1_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_1_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_1_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_1_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_2_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_2_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_2_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_2_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_2_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_2_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_2_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_2_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_2_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_2_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_2_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_2_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_2_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_2_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_2_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_2_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_2_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_2_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_3_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_3_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_3_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_3_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_3_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_3_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_3_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_3_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_3_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_3_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_3_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_3_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_3_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_3_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_3_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_3_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_3_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_3_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_4_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_4_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_4_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_4_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_4_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_4_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_4_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_4_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_4_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_4_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_4_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_4_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_4_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_4_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_4_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_4_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_4_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_4_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_5_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_5_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_5_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_5_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_5_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_5_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_5_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_5_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_5_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_5_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_5_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_5_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_5_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_5_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_5_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_5_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_5_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_5_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_6_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_6_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_6_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_6_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_6_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_6_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_6_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_6_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_6_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_6_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_6_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_6_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_6_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_6_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_6_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_6_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_6_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_6_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_7_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_7_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_7_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_7_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_7_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_7_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_7_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_7_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_7_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_7_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_7_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_7_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_7_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_7_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_7_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_7_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_7_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_7_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_8_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_8_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_8_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_8_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_8_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_8_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_8_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_8_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_8_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_8_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_8_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_8_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_8_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_8_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_8_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_8_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_8_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_8_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_9_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_9_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_9_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_9_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_9_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_9_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_9_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_9_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_9_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_9_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_9_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_9_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_9_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_9_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_9_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_9_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_9_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_9_self_attn_v_proj.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_proj_in.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_proj_in.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_proj_in.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_proj_out.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_proj_out.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_proj_out.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_proj_in.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_proj_in.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_proj_in.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_proj_out.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_proj_out.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_proj_out.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_proj_in.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_proj_in.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_proj_in.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_proj_out.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_proj_out.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_proj_out.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_proj_in.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_proj_in.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_proj_in.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_proj_out.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_proj_out.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_proj_out.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_proj_in.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_proj_in.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_proj_in.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_proj_out.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_proj_out.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_proj_out.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_proj_in.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_proj_in.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_proj_in.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_proj_out.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_proj_out.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_proj_out.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_proj_in.alpha\n",
      "lora_unet_mid_block_attentions_0_proj_in.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_proj_in.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_proj_out.alpha\n",
      "lora_unet_mid_block_attentions_0_proj_out.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_proj_out.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_proj_in.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_proj_in.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_proj_in.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_proj_out.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_proj_out.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_proj_out.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_proj_in.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_proj_in.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_proj_in.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_proj_out.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_proj_out.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_proj_out.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_proj_in.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_proj_in.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_proj_in.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_proj_out.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_proj_out.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_proj_out.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_proj_in.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_proj_in.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_proj_in.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_proj_out.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_proj_out.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_proj_out.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_proj_in.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_proj_in.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_proj_in.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_proj_out.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_proj_out.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_proj_out.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_proj_in.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_proj_in.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_proj_in.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_proj_out.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_proj_out.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_proj_out.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_proj_in.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_proj_in.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_proj_in.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_proj_out.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_proj_out.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_proj_out.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_proj_in.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_proj_in.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_proj_in.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_proj_out.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_proj_out.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_proj_out.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_proj_in.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_proj_in.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_proj_in.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_proj_out.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_proj_out.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_proj_out.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2.lora_up.weight\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from safetensors import safe_open\n",
    "from scipy.linalg import svd\n",
    "import numpy as np\n",
    "\n",
    "# Load the LoRA tensors from .safetensors files\n",
    "with safe_open(\"lora1.safetensors\", framework=\"pt\", device=\"cpu\") as f:\n",
    "    lora1_tensors = {}\n",
    "    for k in f.keys():\n",
    "        lora1_tensors[k] = f.get_tensor(k)\n",
    "\n",
    "# Print the available keys\n",
    "print(\"Keys in lora1_tensors:\")\n",
    "for key in lora1_tensors.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Keys in lora2_tensors:\n",
      "lora_te_text_model_encoder_layers_0_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_0_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_0_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_0_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_0_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_0_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_0_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_0_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_0_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_0_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_0_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_0_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_0_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_0_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_0_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_0_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_0_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_0_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_10_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_10_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_10_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_10_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_10_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_10_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_10_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_10_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_10_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_10_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_10_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_10_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_10_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_10_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_10_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_10_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_10_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_10_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_11_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_11_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_11_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_11_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_11_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_11_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_11_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_11_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_11_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_11_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_11_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_11_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_11_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_11_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_11_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_11_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_11_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_11_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_1_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_1_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_1_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_1_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_1_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_1_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_1_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_1_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_1_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_1_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_1_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_1_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_1_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_1_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_1_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_1_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_1_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_1_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_2_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_2_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_2_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_2_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_2_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_2_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_2_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_2_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_2_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_2_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_2_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_2_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_2_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_2_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_2_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_2_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_2_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_2_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_3_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_3_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_3_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_3_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_3_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_3_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_3_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_3_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_3_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_3_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_3_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_3_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_3_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_3_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_3_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_3_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_3_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_3_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_4_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_4_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_4_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_4_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_4_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_4_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_4_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_4_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_4_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_4_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_4_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_4_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_4_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_4_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_4_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_4_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_4_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_4_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_5_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_5_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_5_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_5_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_5_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_5_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_5_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_5_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_5_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_5_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_5_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_5_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_5_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_5_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_5_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_5_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_5_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_5_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_6_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_6_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_6_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_6_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_6_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_6_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_6_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_6_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_6_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_6_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_6_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_6_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_6_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_6_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_6_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_6_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_6_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_6_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_7_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_7_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_7_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_7_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_7_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_7_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_7_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_7_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_7_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_7_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_7_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_7_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_7_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_7_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_7_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_7_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_7_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_7_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_8_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_8_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_8_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_8_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_8_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_8_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_8_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_8_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_8_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_8_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_8_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_8_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_8_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_8_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_8_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_8_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_8_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_8_self_attn_v_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_9_mlp_fc1.alpha\n",
      "lora_te_text_model_encoder_layers_9_mlp_fc1.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_9_mlp_fc1.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_9_mlp_fc2.alpha\n",
      "lora_te_text_model_encoder_layers_9_mlp_fc2.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_9_mlp_fc2.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_9_self_attn_k_proj.alpha\n",
      "lora_te_text_model_encoder_layers_9_self_attn_k_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_9_self_attn_k_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_9_self_attn_out_proj.alpha\n",
      "lora_te_text_model_encoder_layers_9_self_attn_out_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_9_self_attn_out_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_9_self_attn_q_proj.alpha\n",
      "lora_te_text_model_encoder_layers_9_self_attn_q_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_9_self_attn_q_proj.lora_up.weight\n",
      "lora_te_text_model_encoder_layers_9_self_attn_v_proj.alpha\n",
      "lora_te_text_model_encoder_layers_9_self_attn_v_proj.lora_down.weight\n",
      "lora_te_text_model_encoder_layers_9_self_attn_v_proj.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_proj_in.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_proj_in.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_proj_in.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_proj_out.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_proj_out.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_proj_out.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_proj_in.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_proj_in.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_proj_in.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_proj_out.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_proj_out.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_proj_out.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_proj_in.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_proj_in.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_proj_in.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_proj_out.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_proj_out.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_proj_out.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_proj_in.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_proj_in.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_proj_in.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_proj_out.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_proj_out.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_proj_out.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_proj_in.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_proj_in.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_proj_in.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_proj_out.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_proj_out.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_proj_out.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_proj_in.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_proj_in.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_proj_in.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_proj_out.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_proj_out.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_proj_out.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_proj_in.alpha\n",
      "lora_unet_mid_block_attentions_0_proj_in.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_proj_in.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_proj_out.alpha\n",
      "lora_unet_mid_block_attentions_0_proj_out.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_proj_out.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_proj_in.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_proj_in.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_proj_in.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_proj_out.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_proj_out.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_proj_out.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_proj_in.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_proj_in.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_proj_in.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_proj_out.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_proj_out.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_proj_out.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_proj_in.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_proj_in.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_proj_in.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_proj_out.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_proj_out.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_proj_out.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_proj_in.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_proj_in.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_proj_in.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_proj_out.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_proj_out.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_proj_out.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_proj_in.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_proj_in.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_proj_in.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_proj_out.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_proj_out.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_proj_out.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_proj_in.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_proj_in.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_proj_in.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_proj_out.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_proj_out.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_proj_out.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_proj_in.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_proj_in.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_proj_in.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_proj_out.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_proj_out.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_proj_out.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_proj_in.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_proj_in.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_proj_in.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_proj_out.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_proj_out.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_proj_out.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_proj_in.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_proj_in.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_proj_in.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_proj_out.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_proj_out.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_proj_out.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj.lora_up.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2.alpha\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2.lora_down.weight\n",
      "lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2.lora_up.weight\n"
     ]
    }
   ],
   "source": [
    "with safe_open(\"lora2.safetensors\", framework=\"pt\", device=\"cpu\") as f:\n",
    "    lora2_tensors = {}\n",
    "    for k in f.keys():\n",
    "        lora2_tensors[k] = f.get_tensor(k)\n",
    "\n",
    "print(\"\\nKeys in lora2_tensors:\")\n",
    "for key in lora2_tensors.keys():\n",
    "    print(key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in the paper on LoRAs, the following concept is introduced: The subspace similarity measure is a way of measuring the similarity between the subspaces spanned by the top singular vectors of two low-rank adaptation matrices, $A_{r=8}$ and $A_{r=64}$, from the same pre-trained model. Here's how it's done:\n",
    "\n",
    "First, you perform a singular value decomposition (SVD) on each of these matrices to obtain their right-singular unitary matrices, denoted $U_{A_{r=8}}$ and $U_{A_{r=64}}$.\n",
    "\n",
    "The goal is then to quantify how much of the subspace spanned by the top $i$ singular vectors in $U_{A_{r=8}}$ is contained in the subspace spanned by the top $j$ singular vectors of $U_{A_{r=64}}$.\n",
    "\n",
    "This is measured using a normalized subspace similarity based on the Grassmann distance. The formula for this measure, denoted $\\phi(A_{r=8}, A_{r=64}, i, j)$, is given as follows:\n",
    "\n",
    "$$\n",
    "\\phi(A_{r=8}, A_{r=64}, i, j) = \\frac{||U_{A_{r=8}}^{(i)} {U_{A_{r=64}}^{(j)}}^T||_F^2}{\\min(i, j)}\n",
    "$$\n",
    "\n",
    "where $U_{A_{r}}^{(i)}$ represents the columns of $U_{A_{r}}$ corresponding to the top $i$ singular vectors, and $||\\cdot||_F$ denotes the Frobenius norm.\n",
    "\n",
    "The measure $\\phi()$ ranges from 0 to 1, where 1 represents a complete overlap of the subspaces (i.e., they are the same), and 0 represents a complete separation (i.e., they are orthogonal). This is a normalized measure because it's divided by $\\min(i, j)$, which is the maximum possible square of the Frobenius norm of the product matrix $U_{A_{r=8}}^{(i)} {U_{A_{r=64}}^{(j)}}^T$.\n",
    "\n",
    "This process is performed for all pairs $(i, j)$ where $1 \\leq i \\leq 8$ and $1 \\leq j \\leq 64$. The results give an understanding of how much the learned subspaces for different ranks overlap with each other.\n",
    "\n",
    "This can also be performed on two layers $\\Delta W_1 = B_1A_1$ and $\\Delta W_2 = B_2A_2$  in two different LoRAs. In particular, suppose we choose a layer `n` of each LoRA and run the subspace similarity measure comparison on $U_{\\Delta W_1}^{(i)} {U_{\\Delta W_2}^{(j)}}^T$. Then this will tell us how much those to LoRAs overlap with one another. \n",
    "\n",
    "This could be useful in determining which LoRAs to merge. If we run this analysis on all of the weight matrices of two different LoRAs, then we can determine how much layer `n` of `lora1` overlaps with layer `n` of `lora2`. If the overlap is small, then the two weight martices $\\Delta W_1^{(n)} = B_1^{(n)}A_1^{(n)}$ and $\\Delta W_2^{(n)} = B_2^{(n)}A_2^{(n)}$ may express very different things because the subspaces that they span do not overlap very much. So, to be more explicit, we compute\n",
    "\n",
    "$$\n",
    "\\phi(\\Delta W_1^{(n)}, \\Delta W_2^{(n)}, i, j) = \\frac{||U_{\\Delta W_1^{(n)}}^{(i)} {U_{\\Delta W_2^{(n)}}^{(j)}}^T||_F^2}{\\min(i, j)}\n",
    "$$\n",
    "\n",
    "for a weight matrix $\\Delta W_1^{(n)}$ from the first LoRA, and the corresponding $\\Delta W_2^{(n)}$ from the second LoRA. This could indicate that merging the two LoRAs will create a more general model, able to create a wider range of diverse styles. This might also help in explaining why two LoRAs create something very muddy or undesirable when merges. Obviously, this is all conjecture based on a mathematical analysis that needs to be tested, and it does not provide a precise theshold for the overlap. What upper or lower bound might we use for this subspace similarity measure $\\phi$? Could this hypthesis be wrong, or inverted? That is, is it possible that in some cases we actually want *high* overlap between models so that we merge very similar concepts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The subspace similarity measure is: 0.26528483629226685\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# The A matrices\n",
    "A1 = lora1_tensors['lora_te_text_model_encoder_layers_0_mlp_fc1.lora_down.weight'].float()\n",
    "A2 = lora2_tensors['lora_te_text_model_encoder_layers_0_mlp_fc1.lora_down.weight'].float()\n",
    "\n",
    "# The B matrices\n",
    "B1 = lora1_tensors['lora_te_text_model_encoder_layers_0_mlp_fc1.lora_up.weight'].float()\n",
    "B2 = lora2_tensors['lora_te_text_model_encoder_layers_0_mlp_fc1.lora_up.weight'].float()\n",
    "\n",
    "# Compute the update matrices\n",
    "Delta_W1 = torch.matmul(B1, A1)\n",
    "Delta_W2 = torch.matmul(B2, A2)\n",
    "\n",
    "# Compute the SVD of the update matrices\n",
    "U1, _, _ = torch.svd(Delta_W1)\n",
    "U2, _, _ = torch.svd(Delta_W2)\n",
    "\n",
    "# Define the subspace similarity measure\n",
    "def phi(U1, U2, i, j):\n",
    "    U1_i = U1[:, :i]  # First i columns of U1\n",
    "    U2_j = U2[:, :j]  # First j columns of U2\n",
    "    \n",
    "    product = torch.matmul(U1_i.t(), U2_j)  # Matrix multiplication\n",
    "    norm = torch.norm(product)  # Frobenius norm\n",
    "    \n",
    "    return norm ** 2 / min(i, j)\n",
    "\n",
    "# Calculate the subspace similarity measure\n",
    "i = U1.size(1)  # Number of columns in U1\n",
    "j = U2.size(1)  # Number of columns in U2\n",
    "result = phi(U1, U2, i, j)  # Replace i and j with the desired values\n",
    "\n",
    "print(f'The subspace similarity measure is: {result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([3072, 32])\n",
      "The subspace similarity measure for layer 0 is: 0.26528483629226685\n",
      "A1 shape: torch.Size([32, 3072])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 1 is: 0.9999878406524658\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 2 is: 0.9999885559082031\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 3 is: 0.9999887347221375\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 4 is: 0.9999908804893494\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 5 is: 0.9999882578849792\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([3072, 32])\n",
      "The subspace similarity measure for layer 6 is: 0.26571375131607056\n",
      "A1 shape: torch.Size([32, 3072])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 7 is: 0.999989926815033\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 8 is: 0.9999870657920837\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 9 is: 0.9999893307685852\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 10 is: 0.9999907612800598\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 11 is: 0.9999897480010986\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([3072, 32])\n",
      "The subspace similarity measure for layer 12 is: 0.28491613268852234\n",
      "A1 shape: torch.Size([32, 3072])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 13 is: 0.9999805092811584\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 14 is: 0.9999852180480957\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 15 is: 0.9999846816062927\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 16 is: 0.9999852180480957\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 17 is: 0.9999863505363464\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([3072, 32])\n",
      "The subspace similarity measure for layer 18 is: 0.26683029532432556\n",
      "A1 shape: torch.Size([32, 3072])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 19 is: 0.9999917149543762\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 20 is: 0.9999911785125732\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 21 is: 0.9999918341636658\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 22 is: 0.999991238117218\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 23 is: 0.9999890923500061\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([3072, 32])\n",
      "The subspace similarity measure for layer 24 is: 0.27087515592575073\n",
      "A1 shape: torch.Size([32, 3072])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 25 is: 0.9999900460243225\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 26 is: 0.9999902248382568\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 27 is: 0.9999883770942688\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 28 is: 0.9999927878379822\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 29 is: 0.9999895095825195\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([3072, 32])\n",
      "The subspace similarity measure for layer 30 is: 0.2644103169441223\n",
      "A1 shape: torch.Size([32, 3072])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 31 is: 0.9999893307685852\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 32 is: 0.9999906420707703\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 33 is: 0.9999896883964539\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 34 is: 0.9999893307685852\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 35 is: 0.9999897480010986\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([3072, 32])\n",
      "The subspace similarity measure for layer 36 is: 0.2637903094291687\n",
      "A1 shape: torch.Size([32, 3072])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 37 is: 0.9999907612800598\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 38 is: 0.9999885559082031\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 39 is: 0.9999929070472717\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 40 is: 0.9999893307685852\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 41 is: 0.9999902248382568\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([3072, 32])\n",
      "The subspace similarity measure for layer 42 is: 0.2647744119167328\n",
      "A1 shape: torch.Size([32, 3072])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 43 is: 0.9999902248382568\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 44 is: 0.9999925494194031\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 45 is: 0.9999881386756897\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 46 is: 0.9999892115592957\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 47 is: 0.9999880194664001\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([3072, 32])\n",
      "The subspace similarity measure for layer 48 is: 0.26757362484931946\n",
      "A1 shape: torch.Size([32, 3072])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 49 is: 0.9999914169311523\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 50 is: 0.9999897480010986\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 51 is: 0.9999881386756897\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 52 is: 0.9999889731407166\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 53 is: 0.9999887943267822\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([3072, 32])\n",
      "The subspace similarity measure for layer 54 is: 0.26692089438438416\n",
      "A1 shape: torch.Size([32, 3072])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 55 is: 0.9999887347221375\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 56 is: 0.9999871253967285\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 57 is: 0.9999900460243225\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 58 is: 0.9999880194664001\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 59 is: 0.9999917149543762\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([3072, 32])\n",
      "The subspace similarity measure for layer 60 is: 0.26369643211364746\n",
      "A1 shape: torch.Size([32, 3072])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 61 is: 0.9999907612800598\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 62 is: 0.9999887943267822\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 63 is: 0.9999917149543762\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 64 is: 0.9999876618385315\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 65 is: 0.9999878406524658\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([3072, 32])\n",
      "The subspace similarity measure for layer 66 is: 0.2677040100097656\n",
      "A1 shape: torch.Size([32, 3072])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 67 is: 0.9999887943267822\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 68 is: 0.9999882578849792\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 69 is: 0.9999876022338867\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 70 is: 0.9999895095825195\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([768, 32])\n",
      "The subspace similarity measure for layer 71 is: 0.9999914169311523\n",
      "A1 shape: torch.Size([32, 320, 1, 1])\n",
      "B1 shape: torch.Size([320, 32, 1, 1])\n",
      "Error occurred at layer 72: The size of tensor a (32) must match the size of tensor b (320) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 320, 1, 1])\n",
      "B1 shape: torch.Size([320, 32, 1, 1])\n",
      "Error occurred at layer 73: The size of tensor a (32) must match the size of tensor b (320) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 74 is: 0.9999980926513672\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 75 is: 0.9999991655349731\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 76 is: 1.0\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 77 is: 0.9999987483024597\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 78 is: 0.9999991655349731\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 79 is: 0.9999991655349731\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 80 is: 1.0000004768371582\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 81 is: 0.9999998211860657\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([2560, 32])\n",
      "The subspace similarity measure for layer 82 is: 0.17305810749530792\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 83 is: 1.0\n",
      "A1 shape: torch.Size([32, 320, 1, 1])\n",
      "B1 shape: torch.Size([320, 32, 1, 1])\n",
      "Error occurred at layer 84: The size of tensor a (32) must match the size of tensor b (320) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 320, 1, 1])\n",
      "B1 shape: torch.Size([320, 32, 1, 1])\n",
      "Error occurred at layer 85: The size of tensor a (32) must match the size of tensor b (320) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 86 is: 0.9999994039535522\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 87 is: 0.999998927116394\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 88 is: 0.9999991655349731\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 89 is: 0.9999998211860657\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 90 is: 0.9999987483024597\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 91 is: 0.9999996423721313\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 92 is: 0.9999985694885254\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 93 is: 0.9999996423721313\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([2560, 32])\n",
      "The subspace similarity measure for layer 94 is: 0.1714164912700653\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 95 is: 0.999998927116394\n",
      "A1 shape: torch.Size([32, 640, 1, 1])\n",
      "B1 shape: torch.Size([640, 32, 1, 1])\n",
      "Error occurred at layer 96: The size of tensor a (32) must match the size of tensor b (640) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 640, 1, 1])\n",
      "B1 shape: torch.Size([640, 32, 1, 1])\n",
      "Error occurred at layer 97: The size of tensor a (32) must match the size of tensor b (640) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 98 is: 0.9999935030937195\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 99 is: 0.9999909400939941\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 100 is: 0.9999929666519165\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 101 is: 0.9999938011169434\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 102 is: 0.9999944567680359\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 103 is: 0.9999935030937195\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 104 is: 0.999994158744812\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 105 is: 0.999994158744812\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([5120, 32])\n",
      "The subspace similarity measure for layer 106 is: 0.1543945074081421\n",
      "A1 shape: torch.Size([32, 2560])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 107 is: 0.9999948740005493\n",
      "A1 shape: torch.Size([32, 640, 1, 1])\n",
      "B1 shape: torch.Size([640, 32, 1, 1])\n",
      "Error occurred at layer 108: The size of tensor a (32) must match the size of tensor b (640) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 640, 1, 1])\n",
      "B1 shape: torch.Size([640, 32, 1, 1])\n",
      "Error occurred at layer 109: The size of tensor a (32) must match the size of tensor b (640) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 110 is: 0.9999915957450867\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 111 is: 0.999994158744812\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 112 is: 0.9999936819076538\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 113 is: 0.9999948740005493\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 114 is: 0.9999935030937195\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 115 is: 0.9999935030937195\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 116 is: 0.9999924898147583\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 117 is: 0.9999963641166687\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([5120, 32])\n",
      "The subspace similarity measure for layer 118 is: 0.15738140046596527\n",
      "A1 shape: torch.Size([32, 2560])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 119 is: 0.9999927282333374\n",
      "A1 shape: torch.Size([32, 1280, 1, 1])\n",
      "B1 shape: torch.Size([1280, 32, 1, 1])\n",
      "Error occurred at layer 120: The size of tensor a (32) must match the size of tensor b (1280) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 1280, 1, 1])\n",
      "B1 shape: torch.Size([1280, 32, 1, 1])\n",
      "Error occurred at layer 121: The size of tensor a (32) must match the size of tensor b (1280) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 122 is: 0.9999516606330872\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 123 is: 0.9999529123306274\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 124 is: 0.9999490976333618\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 125 is: 0.9999464750289917\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 126 is: 0.6105306148529053\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 127 is: 0.9999467134475708\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 128 is: 0.9999494552612305\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 129 is: 0.6055203080177307\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([10240, 32])\n",
      "The subspace similarity measure for layer 130 is: 0.1365688443183899\n",
      "A1 shape: torch.Size([32, 5120])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 131 is: 0.9999482035636902\n",
      "A1 shape: torch.Size([32, 1280, 1, 1])\n",
      "B1 shape: torch.Size([1280, 32, 1, 1])\n",
      "Error occurred at layer 132: The size of tensor a (32) must match the size of tensor b (1280) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 1280, 1, 1])\n",
      "B1 shape: torch.Size([1280, 32, 1, 1])\n",
      "Error occurred at layer 133: The size of tensor a (32) must match the size of tensor b (1280) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 134 is: 0.99995356798172\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 135 is: 0.9999462962150574\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 136 is: 0.9999518394470215\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 137 is: 0.999947726726532\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 138 is: 0.6102796196937561\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 139 is: 0.9999510049819946\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 140 is: 0.999951183795929\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 141 is: 0.6098781228065491\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([10240, 32])\n",
      "The subspace similarity measure for layer 142 is: 0.13769392669200897\n",
      "A1 shape: torch.Size([32, 5120])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 143 is: 0.9999533891677856\n",
      "A1 shape: torch.Size([32, 1280, 1, 1])\n",
      "B1 shape: torch.Size([1280, 32, 1, 1])\n",
      "Error occurred at layer 144: The size of tensor a (32) must match the size of tensor b (1280) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 1280, 1, 1])\n",
      "B1 shape: torch.Size([1280, 32, 1, 1])\n",
      "Error occurred at layer 145: The size of tensor a (32) must match the size of tensor b (1280) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 146 is: 0.9999492764472961\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 147 is: 0.9999454617500305\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 148 is: 0.9999492764472961\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 149 is: 0.999952495098114\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 150 is: 0.6096875667572021\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 151 is: 0.999946117401123\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 152 is: 0.9999537467956543\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 153 is: 0.6064993143081665\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([10240, 32])\n",
      "The subspace similarity measure for layer 154 is: 0.1332767903804779\n",
      "A1 shape: torch.Size([32, 5120])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 155 is: 0.9999533891677856\n",
      "A1 shape: torch.Size([32, 1280, 1, 1])\n",
      "B1 shape: torch.Size([1280, 32, 1, 1])\n",
      "Error occurred at layer 156: The size of tensor a (32) must match the size of tensor b (1280) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 1280, 1, 1])\n",
      "B1 shape: torch.Size([1280, 32, 1, 1])\n",
      "Error occurred at layer 157: The size of tensor a (32) must match the size of tensor b (1280) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 158 is: 0.9999464750289917\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 159 is: 0.9999462962150574\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 160 is: 0.9999482035636902\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 161 is: 0.9999533891677856\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 162 is: 0.607451856136322\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 163 is: 0.9999510049819946\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 164 is: 0.9999456405639648\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 165 is: 0.6084207892417908\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([10240, 32])\n",
      "The subspace similarity measure for layer 166 is: 0.13885697722434998\n",
      "A1 shape: torch.Size([32, 5120])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 167 is: 0.9999513626098633\n",
      "A1 shape: torch.Size([32, 1280, 1, 1])\n",
      "B1 shape: torch.Size([1280, 32, 1, 1])\n",
      "Error occurred at layer 168: The size of tensor a (32) must match the size of tensor b (1280) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 1280, 1, 1])\n",
      "B1 shape: torch.Size([1280, 32, 1, 1])\n",
      "Error occurred at layer 169: The size of tensor a (32) must match the size of tensor b (1280) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 170 is: 0.9999482035636902\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 171 is: 0.9999467134475708\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 172 is: 0.9999497532844543\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 173 is: 0.9999483823776245\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 174 is: 0.6065221428871155\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 175 is: 0.999950110912323\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 176 is: 0.999952495098114\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 177 is: 0.6052832007408142\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([10240, 32])\n",
      "The subspace similarity measure for layer 178 is: 0.13806910812854767\n",
      "A1 shape: torch.Size([32, 5120])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 179 is: 0.9999533891677856\n",
      "A1 shape: torch.Size([32, 1280, 1, 1])\n",
      "B1 shape: torch.Size([1280, 32, 1, 1])\n",
      "Error occurred at layer 180: The size of tensor a (32) must match the size of tensor b (1280) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 1280, 1, 1])\n",
      "B1 shape: torch.Size([1280, 32, 1, 1])\n",
      "Error occurred at layer 181: The size of tensor a (32) must match the size of tensor b (1280) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 182 is: 0.9999497532844543\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 183 is: 0.999947726726532\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 184 is: 0.9999490976333618\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 185 is: 0.9999494552612305\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 186 is: 0.6104145646095276\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 187 is: 0.9999492764472961\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 188 is: 0.9999518394470215\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 189 is: 0.6065794229507446\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([10240, 32])\n",
      "The subspace similarity measure for layer 190 is: 0.13888858258724213\n",
      "A1 shape: torch.Size([32, 5120])\n",
      "B1 shape: torch.Size([1280, 32])\n",
      "The subspace similarity measure for layer 191 is: 0.9999510049819946\n",
      "A1 shape: torch.Size([32, 640, 1, 1])\n",
      "B1 shape: torch.Size([640, 32, 1, 1])\n",
      "Error occurred at layer 192: The size of tensor a (32) must match the size of tensor b (640) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 640, 1, 1])\n",
      "B1 shape: torch.Size([640, 32, 1, 1])\n",
      "Error occurred at layer 193: The size of tensor a (32) must match the size of tensor b (640) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 194 is: 0.999995231628418\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 195 is: 0.9999944567680359\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 196 is: 0.9999953508377075\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 197 is: 0.9999927282333374\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 198 is: 0.999996542930603\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 199 is: 0.9999920129776001\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 200 is: 0.9999912977218628\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 201 is: 0.9999947547912598\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([5120, 32])\n",
      "The subspace similarity measure for layer 202 is: 0.16359271109104156\n",
      "A1 shape: torch.Size([32, 2560])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 203 is: 0.9999944567680359\n",
      "A1 shape: torch.Size([32, 640, 1, 1])\n",
      "B1 shape: torch.Size([640, 32, 1, 1])\n",
      "Error occurred at layer 204: The size of tensor a (32) must match the size of tensor b (640) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 640, 1, 1])\n",
      "B1 shape: torch.Size([640, 32, 1, 1])\n",
      "Error occurred at layer 205: The size of tensor a (32) must match the size of tensor b (640) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 206 is: 0.9999931454658508\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 207 is: 0.9999938011169434\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 208 is: 0.9999936819076538\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 209 is: 0.9999963045120239\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 210 is: 0.9999942779541016\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 211 is: 0.9999939799308777\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 212 is: 0.9999924898147583\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 213 is: 0.9999938011169434\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([5120, 32])\n",
      "The subspace similarity measure for layer 214 is: 0.1611660271883011\n",
      "A1 shape: torch.Size([32, 2560])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 215 is: 0.999994158744812\n",
      "A1 shape: torch.Size([32, 640, 1, 1])\n",
      "B1 shape: torch.Size([640, 32, 1, 1])\n",
      "Error occurred at layer 216: The size of tensor a (32) must match the size of tensor b (640) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 640, 1, 1])\n",
      "B1 shape: torch.Size([640, 32, 1, 1])\n",
      "Error occurred at layer 217: The size of tensor a (32) must match the size of tensor b (640) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 218 is: 0.9999934434890747\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 219 is: 0.9999917149543762\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 220 is: 0.9999934434890747\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 221 is: 0.9999942779541016\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 222 is: 0.9999932050704956\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 223 is: 0.9999909400939941\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 224 is: 0.999995231628418\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 225 is: 0.9999920129776001\n",
      "A1 shape: torch.Size([32, 640])\n",
      "B1 shape: torch.Size([5120, 32])\n",
      "The subspace similarity measure for layer 226 is: 0.16014735400676727\n",
      "A1 shape: torch.Size([32, 2560])\n",
      "B1 shape: torch.Size([640, 32])\n",
      "The subspace similarity measure for layer 227 is: 0.9999931454658508\n",
      "A1 shape: torch.Size([32, 320, 1, 1])\n",
      "B1 shape: torch.Size([320, 32, 1, 1])\n",
      "Error occurred at layer 228: The size of tensor a (32) must match the size of tensor b (320) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 320, 1, 1])\n",
      "B1 shape: torch.Size([320, 32, 1, 1])\n",
      "Error occurred at layer 229: The size of tensor a (32) must match the size of tensor b (320) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 230 is: 0.9999991655349731\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 231 is: 0.9999982714653015\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 232 is: 0.9999987483024597\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 233 is: 0.9999998211860657\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 234 is: 0.9999980926513672\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 235 is: 0.9999998211860657\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 236 is: 1.0\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 237 is: 0.9999987483024597\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([2560, 32])\n",
      "The subspace similarity measure for layer 238 is: 0.1775144338607788\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 239 is: 0.9999994039535522\n",
      "A1 shape: torch.Size([32, 320, 1, 1])\n",
      "B1 shape: torch.Size([320, 32, 1, 1])\n",
      "Error occurred at layer 240: The size of tensor a (32) must match the size of tensor b (320) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 320, 1, 1])\n",
      "B1 shape: torch.Size([320, 32, 1, 1])\n",
      "Error occurred at layer 241: The size of tensor a (32) must match the size of tensor b (320) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 242 is: 0.9999998211860657\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 243 is: 0.9999996423721313\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 244 is: 0.9999994039535522\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 245 is: 0.9999998211860657\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 246 is: 0.9999985694885254\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 247 is: 0.9999982714653015\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 248 is: 0.9999998211860657\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 249 is: 0.9999996423721313\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([2560, 32])\n",
      "The subspace similarity measure for layer 250 is: 0.18077997863292694\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 251 is: 0.999998927116394\n",
      "A1 shape: torch.Size([32, 320, 1, 1])\n",
      "B1 shape: torch.Size([320, 32, 1, 1])\n",
      "Error occurred at layer 252: The size of tensor a (32) must match the size of tensor b (320) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 320, 1, 1])\n",
      "B1 shape: torch.Size([320, 32, 1, 1])\n",
      "Error occurred at layer 253: The size of tensor a (32) must match the size of tensor b (320) at non-singleton dimension 1\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 254 is: 0.9999987483024597\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 255 is: 0.9999987483024597\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 256 is: 1.0\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 257 is: 0.9999998211860657\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 258 is: 0.9999987483024597\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 259 is: 0.9999987483024597\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 260 is: 0.9999987483024597\n",
      "A1 shape: torch.Size([32, 768])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 261 is: 0.999998927116394\n",
      "A1 shape: torch.Size([32, 320])\n",
      "B1 shape: torch.Size([2560, 32])\n",
      "The subspace similarity measure for layer 262 is: 0.17570427060127258\n",
      "A1 shape: torch.Size([32, 1280])\n",
      "B1 shape: torch.Size([320, 32])\n",
      "The subspace similarity measure for layer 263 is: 0.9999998211860657\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Gather all keys and sort them\n",
    "all_keys = sorted(list(lora1_tensors.keys()))\n",
    "\n",
    "# Filter keys for lora_down and lora_up pairs\n",
    "lora_down_keys = [key for key in all_keys if 'lora_down' in key]\n",
    "lora_up_keys = [key for key in all_keys if 'lora_up' in key]\n",
    "\n",
    "# Ensure we have matching pairs of keys\n",
    "assert len(lora_down_keys) == len(lora_up_keys), \"Mismatch in number of 'lora_down' and 'lora_up' keys\"\n",
    "\n",
    "# Define the subspace similarity measure\n",
    "def phi(U1, U2, i, j):\n",
    "    U1_i = U1[:, :i]  # First i columns of U1\n",
    "    U2_j = U2[:, :j]  # First j columns of U2\n",
    "    \n",
    "    product = torch.matmul(U1_i.t(), U2_j)  # Matrix multiplication\n",
    "    norm = torch.norm(product)  # Frobenius norm\n",
    "    \n",
    "    return norm ** 2 / min(i, j)\n",
    "\n",
    "# Iterate over all layers\n",
    "for layer in range(len(lora_down_keys)):\n",
    "    try:\n",
    "        # Extract the corresponding A and B matrices\n",
    "        A1 = lora1_tensors[lora_down_keys[layer]].float()\n",
    "        B1 = lora1_tensors[lora_up_keys[layer]].float()\n",
    "\n",
    "        A2 = lora2_tensors[lora_down_keys[layer]].float()\n",
    "        B2 = lora2_tensors[lora_up_keys[layer]].float()\n",
    "\n",
    "        # Print the shapes of A1 and B1 matrices for troubleshooting\n",
    "        print(f\"A1 shape: {A1.shape}\")\n",
    "        print(f\"B1 shape: {B1.shape}\")\n",
    "\n",
    "        # Compute the update matrices\n",
    "        Delta_W1 = torch.matmul(B1, A1)\n",
    "        Delta_W2 = torch.matmul(B2, A2)\n",
    "\n",
    "        # Compute the SVD of the update matrices\n",
    "        U1, _, _ = torch.svd(Delta_W1)\n",
    "        U2, _, _ = torch.svd(Delta_W2)\n",
    "\n",
    "        # Calculate the subspace similarity measure\n",
    "        i = U1.size(1)  # Number of columns in U1\n",
    "        j = U2.size(1)  # Number of columns in U2\n",
    "        result = phi(U1, U2, i, j)  # Replace i and j with the desired values\n",
    "\n",
    "        print(f\"The subspace similarity measure for layer {layer} is: {result}\")\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        # Print the layer number and the error message\n",
    "        print(f\"Error occurred at layer {layer}: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grassmannian (Frchet/Karcher) Mean For Square Update Matrices in two LoRA Models\n",
    "\n",
    "The following is supposed to implement the Frchet (a.k.a. Karcher) mean of two points on the Grassmannian. If we know that the matrices are square, as most of the weight matrices in Stable Diffusion are (and therefore so are the update matrices of LoRA models), then we can use it to compute the Frchet mean of any of the square matrices for two LoRAs. In particular, we can compute the Frchet mean of two update weight matrices $\\Delta W_1^{(n)}$ and $\\Delta W_2^{(n)}$ for some fixed layer `n` of the LoRA models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import svd\n",
    "\n",
    "def grassmann_mean(points, max_iters=100, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Computes the Frchet mean of a set of points on the Grassmann manifold.\n",
    "\n",
    "    Parameters:\n",
    "    - points: a list of numpy arrays, each representing a point on the Grassmannian.\n",
    "    - max_iters: the maximum number of iterations for the algorithm.\n",
    "    - tol: the tolerance for convergence.\n",
    "\n",
    "    Returns:\n",
    "    - The Frchet mean as a numpy array.\n",
    "    \"\"\"\n",
    "    # Initialize the mean to the first point\n",
    "    mean = np.linalg.qr(points[0])[0]\n",
    "    for _ in range(max_iters):\n",
    "        # Compute the tangent space at the current mean\n",
    "        tangent_space = sum([log_map(mean, np.linalg.qr(p)[0]) for p in points]) / len(points)\n",
    "        # Update the mean\n",
    "        new_mean = exp_map(mean, tangent_space)\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(new_mean - mean) < tol:\n",
    "            return new_mean\n",
    "        mean = new_mean\n",
    "    return mean\n",
    "\n",
    "def log_map(p, q):\n",
    "    \"\"\"\n",
    "    Computes the logarithm map at p of q, i.e., the tangent vector at p that points towards q.\n",
    "\n",
    "    Parameters:\n",
    "    - p: a numpy array representing a point on the Grassmannian.\n",
    "    - q: a numpy array representing a point on the Grassmannian.\n",
    "\n",
    "    Returns:\n",
    "    - The tangent vector at p that points towards q.\n",
    "    \"\"\"\n",
    "    # Compute the SVD of p^T q\n",
    "    u, s, vt = svd(np.dot(p.T, q))\n",
    "    # Clip the singular values to the range [-1, 1]\n",
    "    s = np.clip(s, -1, 1)\n",
    "    # Compute the logarithm map\n",
    "    return np.dot(p, np.dot(u, np.dot(np.diag(np.arccos(s)), vt)))\n",
    "\n",
    "\n",
    "def exp_map(p, x):\n",
    "    \"\"\"\n",
    "    Computes the exponential map at p of x, i.e., the point on the Grassmannian reached by following the geodesic in the direction of x from p.\n",
    "\n",
    "    Parameters:\n",
    "    - p: a numpy array representing a point on the Grassmannian.\n",
    "    - x: a numpy array representing a tangent vector at p.\n",
    "\n",
    "    Returns:\n",
    "    - The point on the Grassmannian reached by following the geodesic in the direction of x from p.\n",
    "    \"\"\"\n",
    "    # Compute the SVD of x\n",
    "    u, s, vt = svd(x)\n",
    "    # Compute the exponential map\n",
    "    return np.dot(p, np.dot(u, np.dot(np.diag(np.cos(s)), vt))) + np.dot(u, np.dot(np.diag(np.sin(s)), vt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.41942679  0.55888978 -0.84660875]\n",
      " [ 0.2804597  -0.98964856 -0.45203556]\n",
      " [-0.9643001   0.02577428 -0.5265871 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_random_orthogonal_matrix(dim):\n",
    "    \"\"\"\n",
    "    Generates a random orthogonal matrix of the given dimension.\n",
    "\n",
    "    Parameters:\n",
    "    - dim: the dimension of the matrix.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array representing an orthogonal matrix of the given dimension.\n",
    "    \"\"\"\n",
    "    # Generate a random matrix\n",
    "    mat = np.random.randn(dim, dim)\n",
    "    # Compute the QR decomposition of the matrix\n",
    "    q, _ = np.linalg.qr(mat)\n",
    "    return q\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate a list of points on the Grassmannian\n",
    "points = [generate_random_orthogonal_matrix(3) for _ in range(5)]\n",
    "\n",
    "# Compute the Frchet mean of the points\n",
    "mean = grassmann_mean(points)\n",
    "\n",
    "print(mean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
